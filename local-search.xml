<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>布斯算法求补码乘积</title>
    <link href="/2022/10/04/%E5%B8%83%E6%96%AF%E7%AE%97%E6%B3%95%E6%B1%82%E8%A1%A5%E7%A0%81%E4%B9%98%E7%A7%AF/"/>
    <url>/2022/10/04/%E5%B8%83%E6%96%AF%E7%AE%97%E6%B3%95%E6%B1%82%E8%A1%A5%E7%A0%81%E4%B9%98%E7%A7%AF/</url>
    
    <content type="html"><![CDATA[<p>不知道是上课没认真听讲，还是老师和课件讲得实在烂，反正我是被课件和网上的介绍给弄得很糊涂。明明很简单的算法，搞什么花里胡哨的来糊弄人。</p><span id="more"></span><h2 id="布斯算法">布斯算法</h2><p>贴一个网上的介绍：</p><p><img src="/img/article_content/2022-10-04/0.png" alt=""></p><p>说得很好，但是第三点，为什么要算术右移？部分积又是什么东西？根本没解释清楚。</p><h2 id="正确的计算过程">正确的计算过程</h2><p>举例2 * (-5)。首先规定下乘数和被乘数。这里以2为被乘数，-5为乘数。</p><p>先写出被乘数及其相反数的补码，以及乘数的补码。要写多少位呢？这是一个问题，事实上<strong>不是很重要</strong>，这是因为之后乘数是要改写的，从布斯算法的第二点可以看到11是不做任何操作的。但是要考虑溢出问题，就是补码的范围正负是不对称的，最小的那个数的相反数<strong>不在正数范围里</strong>，因此被乘数至少要是<strong>双符号位</strong>。其他应该不需要注意了，但我看网上一般都是把两个乘数的位数补齐。</p><p>-2：1110，2：0010，-5：1011。</p><p>然后把乘数改动，每一位变成前一位减当前位，则成为：(-1) 1 0 (-1)</p><p>然后直接相乘，-1时用-2的补码，1时用2的补码：</p><p><img src="/img/article_content/2022-10-04/2.png" alt=""></p><p>记得在加之前补齐符号位</p><p><img src="/img/article_content/2022-10-04/3.png" alt=""></p><p>结束。</p><p>可以看到布斯算法的思想在这种呈现方式中展现得淋漓尽致，就是把两位合成为一位，减少计算量。</p><p>可能有个bug就是在加到最后的时候溢出，此时要注意一下这个溢出会不会改变符号位，如果会应该多写几位。</p><p>但是这里根本就没什么右移，只有左移；也不知道部分积在哪。</p><h2 id="课件上莫名其妙的解释">课件上莫名其妙的解释</h2><p><img src="/img/article_content/2022-10-04/1.png" alt="课件上的截图"></p><p>上图的意思如下：乘数和被乘数固定为5位，然后红色是乘数，所谓右移，<strong>其实是乘数的右移</strong>！黑色部分右移了吗？右移了，但没有完全右移。你看它符号位倒是多了，但黑色的个位并没有因为右移而被抹去。</p><p>上图中黑色和红色其实是可以分开，但写在一起是为了迎合所谓的“右移”。注意在加的时候（比如第三步），X和-X永远是5位，事实上是它们左移了，而不是部分积右移！</p><p>所以我不知道为什么要这样教，难道这能体现出布斯算法的思想？抑或是这样更容易让早八的大学生懂？？？？？？？我实在是不理解。</p>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
      <category>计算机组成原理</category>
      
    </categories>
    
    
    <tags>
      
      <tag>零碎知识点</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>因果推断导论笔记-Lecture2-Assignment Mechanism</title>
    <link href="/2022/09/26/%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD%E5%AF%BC%E8%AE%BA%E7%AC%94%E8%AE%B0-Lecture2-Assignment%20Mechanism/"/>
    <url>/2022/09/26/%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD%E5%AF%BC%E8%AE%BA%E7%AC%94%E8%AE%B0-Lecture2-Assignment%20Mechanism/</url>
    
    <content type="html"><![CDATA[<p>分配机制的介绍。分配机制对因果推断非常重要。</p><span id="more"></span><h2 id="一些记号">一些记号</h2><p>共五个元素：Unit，Treatment，Potential outcomes，X（协变量），W（分配）。</p><p>随机性来源于W，W是一个随机变量（可取0取1）。</p><p>分配机制，就是要研究<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>W</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(W)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mclose">)</span></span></span></span></p><p>平均因果作用（ACE），average causal effect，定义为：</p><p class="katex-block "><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>τ</mi><mrow><mi>f</mi><mi>s</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><mo stretchy="false">(</mo><msub><mi>Y</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo><mo>−</mo><msub><mi>Y</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">τ_{fs}=\frac{1}{N}\sum\limits_{i=1}\limits^{N}{(Y_i(1)-Y_i(0))}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.1132em;">τ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1132em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10764em;">f</span><span class="mord mathnormal mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.106em;vertical-align:-1.2777em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">1</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">0</span><span class="mclose">))</span></span></span></span></span></span></p><p>观测值、缺失值相当于一个复合的映射：</p><p class="katex-block "><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msubsup><mi>Y</mi><mi>i</mi><mrow><mi>o</mi><mi>b</mi><mi>s</mi></mrow></msubsup><mo>=</mo><msub><mi>Y</mi><mi>i</mi></msub><mo stretchy="false">(</mo><msub><mi>W</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msub><mi>W</mi><mi>i</mi></msub><msub><mi>Y</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><msub><mi>W</mi><mi>i</mi></msub><mo stretchy="false">)</mo><msub><mi>Y</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Y_i^{obs}=Y_i(W_i)=W_iY_i(1)+(1-W_i)Y_i(0)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1461em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8991em;"><span style="top:-2.453em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">b</span><span class="mord mathnormal mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">1</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">0</span><span class="mclose">)</span></span></span></span></span></p><p class="katex-block "><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msubsup><mi>Y</mi><mi>i</mi><mrow><mi>m</mi><mi>i</mi><mi>s</mi></mrow></msubsup><mo>=</mo><msub><mi>Y</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><msub><mi>W</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><msub><mi>W</mi><mi>i</mi></msub><mo stretchy="false">)</mo><msub><mi>Y</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo><mo>+</mo><msub><mi>W</mi><mi>i</mi></msub><msub><mi>Y</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Y_i^{mis}=Y_i(1-W_i)=(1-W_i)Y_i(1)+W_iY_i(0)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1217em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8747em;"><span style="top:-2.453em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">mi</span><span class="mord mathnormal mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">1</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">0</span><span class="mclose">)</span></span></span></span></span></p><h2 id="Simpson’s-Paradox">Simpson’s Paradox</h2><p>New Jersey州的考试成绩比Nebraska的考试成绩低，但是分人种的话全部都是New Jersey州占优。</p><p>这是因为New Jersey州的白人占比比较少，而白人的平均成绩会比较高。</p><p>所以如果加权来看（用国家的人种比例统一加权），New Jersey州的成绩更好。</p><p>选择哪种计算方式，Depends on your questions。</p><p><img src="/img/article_content/2022-09-26/0.png" alt="考试成绩图"></p><p>人种在这里就是<strong>协变量</strong>，不是我们关心的因果变量。</p><p>因此可以看出分配机制不仅和潜在结果有关，还和协变量有关。</p><h2 id="分配机制的精确定义">分配机制的精确定义</h2><h3 id="分配机制">分配机制</h3><p>分配机制是一个随机向量。有n个units，分配机制就是n维，共有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>2</mn><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">2^n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6644em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span></span></span></span></span></span></span>种取值。</p><p><img src="/img/article_content/2022-09-26/1.png" alt="有总体水平和个体水平之分，个体水平有点像边缘概率"></p><p>两种处理的协变量比例要相同。</p><h3 id="倾向得分Propensity-Score">倾向得分Propensity Score</h3><p><img src="/img/article_content/2022-09-26/2.png" alt=""></p><p>相当于边缘概率的另外一个方向，一个子总体内部有多高的比例分配到处理组。</p><p>从另一个角度，倾向得分可以作更粗的划分，使得样本量增大。</p><h3 id="通过例子理解">通过例子理解</h3><p><img src="/img/article_content/2022-09-26/3.png" alt="本例子的分配机制要求必须一个处理一个控制"></p><p><img src="/img/article_content/2022-09-26/4.png" alt="有多个“平行世界”的分配"></p><p>下面这个例子是更复杂的分配机制，unit1随机，unit2和unit1相反，unit3取决于哪个结果更好（不考虑结果相等）：</p><p><img src="/img/article_content/2022-09-26/5.png" alt=""></p><p>进入某个<strong>平行世界</strong>后，Y就没有随机性了。分析过程可以先画序贯树：</p><p><img src="/img/article_content/2022-09-26/6.png" alt=""></p><p>结果如下：</p><p><img src="/img/article_content/2022-09-26/7.png" alt=""></p><h3 id="假设">假设</h3><p>从上面可以看出分配机制很复杂，需要增加一些假设来简化。</p><h4 id="个体化假设">个体化假设</h4><p>上述<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">p_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>要取决于其他个体的潜在结果。</p><p>因此假设为：一个个体的分配概率和其他协变量、其他个体的潜在结果无关，且如果协变量、潜在结果相同，概率相同。还假设了向量的联合概率分布可以从边缘的概率表示。</p><p><img src="/img/article_content/2022-09-26/8.png" alt=""></p><p>这样，倾向得分化简为：</p><p class="katex-block "><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>e</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><msub><mi>N</mi><mi>x</mi></msub></mfrac><munder><mo>∑</mo><mrow><mi>i</mi><mo>:</mo><msub><mi>X</mi><mi>i</mi></msub><mo>=</mo><mi>x</mi></mrow></munder><mi>q</mi><mo stretchy="false">(</mo><msub><mi>X</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>Y</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo><mo separator="true">,</mo><msub><mi>Y</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">e(x)=\frac{1}{N_x}\sum_{i:X_i=x}q(X_i,Y_i(0),Y_i(1))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">e</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.7159em;vertical-align:-1.3944em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.836em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.8557em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">:</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em;"><span style="top:-2.357em;margin-left:-0.0785em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mrel mtight">=</span><span class="mord mathnormal mtight">x</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3944em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">0</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">1</span><span class="mclose">))</span></span></span></span></span></p><h4 id="概率性分配">概率性分配</h4><p>probabilistic.</p><p>简单来讲就是每一项倾向得分都要在(0, 1)之间。</p><h4 id="无混杂假设">无混杂假设</h4><p>在Simpson悖论中，Y是通过影响X来影响X的，平衡了X，Y就没影响了。</p><p>因此假设在X条件下，W和Y独立。</p><p><img src="/img/article_content/2022-09-26/9.png" alt=""></p><p>这样，倾向得分就化简成了：</p><p class="katex-block "><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>e</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>q</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">e(x)=q(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">e</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span></p><blockquote><p>如果无混杂假设不成立怎么办？</p><p>可以尝试增加X，使其达到无混杂。</p><p>但是也不是所有X都加进来，有可能多加一个X就不独立了。</p></blockquote><p>以上三种假设称为强可忽略性假设。</p><p>普通的可忽略性假设指的是，分配机制可以不依赖于看不到的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Y</mi><mrow><mi>m</mi><mi>i</mi><mi>s</mi></mrow></msup></mrow><annotation encoding="application/x-tex">Y^{mis}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8247em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8247em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">mi</span><span class="mord mathnormal mtight">s</span></span></span></span></span></span></span></span></span></span></span></span>了，只取决于<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mo separator="true">,</mo><mi>X</mi><mo separator="true">,</mo><msup><mi>Y</mi><mrow><mi>o</mi><mi>b</mi><mi>s</mi></mrow></msup></mrow><annotation encoding="application/x-tex">W,X,Y^{obs}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0435em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">b</span><span class="mord mathnormal mtight">s</span></span></span></span></span></span></span></span></span></span></span></span>。</p><p>上面三条假设和SUTVA<strong>没有直接的强弱关系</strong>，SUTVA是在客观的没有随机性的世界里的假设，而上述三条假设是在分配机制上的假设。但<strong>有关联</strong>，因为是在SUTVA的基础上完成的。</p><h2 id="两种data-collection">两种data collection</h2><h3 id="Randomized-Experiments">Randomized Experiments</h3><p>an assignment mechanism that</p><ol><li>is probabilistic, and</li><li>has a known functional form that is controlled by the researcher.</li></ol><p>经典的随机化试验还要求：</p><ol><li>individualistic and</li><li>unconfounded</li></ol><p>经典随机化试验有四种特例，区别就是支撑集<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>W</mi><mo>+</mo></msup></mrow><annotation encoding="application/x-tex">W^+</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7713em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7713em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">+</span></span></span></span></span></span></span></span></span></span></span>（真实可能取到的集合）：</p><h4 id="Bernoulli-Trails">Bernoulli Trails</h4><p>按概率独立地随机抽：</p><p><img src="/img/article_content/2022-09-26/10.png" alt=""></p><h4 id="Completely-Randomized-Experiment">Completely Randomized Experiment</h4><p>数量要固定：</p><p><img src="/img/article_content/2022-09-26/11.png" alt=""></p><p>此外还有Stratified Randomized Experiment和Paired Randomized Design。</p><h3 id="Observational">Observational</h3><p>the functional form of <strong>the assignment mechanism is unknown</strong>(the key difference from experiments).</p><p>要求是规则的，即满足：</p><ol><li>individualistic</li><li>probabilistic</li><li>unconfounded</li></ol>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
      <category>因果推断导论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>统计</tag>
      
      <tag>因果推断</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>因果推断导论笔记-Lecture1-The Neyman-Rubin Framework</title>
    <link href="/2022/09/22/%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD%E5%AF%BC%E8%AE%BA%E7%AC%94%E8%AE%B0-Lecture1-The%20Neyman-Rubin%20Framework/"/>
    <url>/2022/09/22/%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD%E5%AF%BC%E8%AE%BA%E7%AC%94%E8%AE%B0-Lecture1-The%20Neyman-Rubin%20Framework/</url>
    
    <content type="html"><![CDATA[<p>这门课终于手选成功了！之前立flag手选成功就要做好笔记，而且现在觉得统辅的笔记需要记录下来整理好，不像专业课在之后会时时用到，像上学期学的多元和回归都忘光了（</p><span id="more"></span><h2 id="Introduction">Introduction</h2><h3 id="因果推断的难点">因果推断的难点</h3><p>相关性 + 前后顺序 ≠ 因果，例如公鸡打鸣-&gt;太阳升起</p><p>因果也不是“相关”的更强结论：采矿工人的平均寿命与普通人几乎相同（在数据上看不出相关性，问题出在采矿工人本身就是年富力强的人）。</p><p>但<strong>在一定的假设下</strong>，可以从相关认识因果。</p><h3 id="因果推断的目的">因果推断的目的</h3><p>1.<strong>关系</strong>：存在性、关系网。</p><p>2.<strong>因果作用</strong>：两个变量之间的因果关系的强弱。</p><p>3.发生因果的机制（其他学科的内容）</p><h3 id="Data-collection">Data collection</h3><p>观察性数据、实验性数据。</p><p>观察性数据的获取更难。</p><h3 id="框架">框架</h3><p>两套：统计学的Neyman-Rubin框架，计算机的Robins框架</p><h2 id="Neyman-Rubin-Framework">Neyman-Rubin Framework</h2><p>条件概率P(Y | X)是因果吗？</p><ul><li>因果是对比，同一个个体（unit）在不同原因（action）下的不同结果。</li><li>而条件概率，X=1和X=2时，已经是不同的范围了。</li></ul><h3 id="定义">定义</h3><p><strong>Unit</strong>：可以理解为个体。</p><p><strong>Action</strong>：a manipulation, treatment or intervention that can be potentially applied to a unit . 例如吃药等。本课重点关注二元的Action，即treatment和control。</p><p><strong>The key idea is that causality is tied to an action applied to a unit.</strong></p><p>而把结果Outcome视为映射的结果，这种结果称为潜在结果（potential outcome），在action和unit确定的情况下，潜在结果固定。</p><p>因果效应定义为两种潜在结果的比较：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Y</mi><mi>t</mi></msub><mo>−</mo><msub><mi>Y</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">Y_t - Y_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>或<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Y</mi><mi>t</mi></msub><mi mathvariant="normal">/</mi><msub><mi>Y</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">Y_t / Y_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">/</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><br>S</p><h3 id="Challenge-Solution">Challenge &amp; Solution</h3><p>真实世界只能看到<strong>一个</strong>潜在结果，也就是说，我们注定会缺失数据。</p><p>个体的处理之间可能有关系，比如两位病人、分别是否吃阿司匹林，那么共有4种处理，导致了有4种潜在结果，导致了有6种比较。</p><p><em>因此增加假设/约束 SUTVA：</em></p><ol><li><p>No Interference：个体间没有干涉，即<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Y</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mover accent="true"><mi>t</mi><mo>⃗</mo></mover><mo stretchy="false">)</mo><mo>=</mo><msub><mi>Y</mi><mi>i</mi></msub><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Y_i( \vec{t})=Y_i(t_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1481em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8981em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">t</span></span><span style="top:-3.1841em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1522em;"><span class="overlay" style="height:0.714em;width:0.471em;"><svg xmlns="http://www.w3.org/2000/svg" width='0.471em' height='0.714em' style='width:0.471em' viewBox='0 0 471 714' preserveAspectRatio='xMinYMin'><path d='M377 20c0-5.333 1.833-10 5.5-14S391 0 397 0c4.667 0 8.667 1.667 12 53.333 2.667 6.667 9 10 19 6.667 24.667 20.333 43.667 41 57 7.333 4.667 1110.667 11 18 0 6-1 10-3 12s-6.667 5-14 9c-28.667 14.667-53.667 35.667-75 63-1.333 1.333-3.167 3.5-5.5 6.5s-4 4.833-5 5.5c-1 .667-2.5 1.333-4.5 2s-4.333 1-7 1c-4.667 0-9.167-1.833-13.5-5.5S337 184 337 178c0-12.667 15.667-32.333 47-59H213l-171-1c-8.667-6-13-12.333-13-19 0-4.667 4.333-11.333 13-20h359c-16-25.333-24-45-24-59z'/></svg></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></p><blockquote><p>注意，这不是说处理相同，个体不同则结果相同！！之前理解错了，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">Y_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是各不相同的。</p></blockquote></li><li><p>No Hidden Variations of Treatments：处理只有明确的两种，即<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Y</mi><mi>i</mi></msub><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>∈</mo><mo stretchy="false">{</mo><msub><mi>Y</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><msub><mi>Y</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">Y_i(t_i)∈\{Y_i(t),Y_i(c)\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">c</span><span class="mclose">)}</span></span></span></span>。</p></li></ol><p>SUTVA是关于Action和Potential outcomes的假设。</p><p><em>Q : 潜在结果是随机变量吗？</em></p><p>不是，映射本身不具有随机性，这里先认为潜在结果是确定的，随机性来自于action的选择。（随机变量是样本空间到实数轴的映射）</p><h3 id="分配机制">分配机制</h3><p>示意如图，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">W_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>有p的概率为0，有1-p的概率为1.</p><p><img src="/img/article_content/2022-09-22/0.png" alt=""></p><p>分配机制是否合理会直接导致结果是否正确。</p>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
      <category>因果推断导论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>统计</tag>
      
      <tag>因果推断</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kaggle首战记录(5)-English Language Learning-cohesion维度的单独训练</title>
    <link href="/2022/09/12/Kaggle%E9%A6%96%E6%88%98%E8%AE%B0%E5%BD%95(5)-English%20Language%20Learning-cohesion%E7%BB%B4%E5%BA%A6%E7%9A%84%E5%8D%95%E7%8B%AC%E8%AE%AD%E7%BB%83/"/>
    <url>/2022/09/12/Kaggle%E9%A6%96%E6%88%98%E8%AE%B0%E5%BD%95(5)-English%20Language%20Learning-cohesion%E7%BB%B4%E5%BA%A6%E7%9A%84%E5%8D%95%E7%8B%AC%E8%AE%AD%E7%BB%83/</url>
    
    <content type="html"><![CDATA[<p>前文提到了多个维度一同训练的弊端，包括数据增强的难处，以及不同维度相互牵制使得性能下降。本文单独处理了cohesion看看效果（由于batchsize无法很大，把batchnorm换成了dropout），效果印证了前文的猜测。</p><span id="more"></span><h2 id="cohesion是什么？">cohesion是什么？</h2><p>说实话，只看英文翻译根本看不出来cohesion到底想要评什么分。</p><p><img src="/img/article_content/2022-09-12/2.png" alt=""></p><p>然后在网上查了一下，发现雅思有这个评分标准：</p><p><a href="https://zhuanlan.zhihu.com/p/379063611">雅思写作评分标准之Cohesion（上）-明衔接 - 知乎 (zhihu.com)</a></p><p>大概意思是链接顺畅，连接词合理，使用代词，名词替换合理（最后这个我在实验前没看仔细，所以后文的处理方法有一丢丢小问题qaq，但是效果有提升就是了）。</p><h2 id="数据增强">数据增强</h2><p>依照这个思路，理出数据增强的方法：</p><p>1.适当破坏或替换动词和名词（其实！正确方法应该是针对动词就行了，因为cohesion和连接词主要是副词、名词和代词都有关系！）</p><p>2.以句子为单位的窗口滑动，但要保证窗口不过于小以至于评分不匹配。</p><h3 id="代码">代码</h3><p>简单地以段落为单位EDA一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br>data = pd.read_csv(<span class="hljs-string">&#x27;./train.csv&#x27;</span>)<br>data = data[:<span class="hljs-number">3750</span>]<br><span class="hljs-keyword">import</span> re<br><br>rx = <span class="hljs-string">r&#x27;\n\n&#x27;</span><br>df = data[<span class="hljs-string">&quot;full_text&quot;</span>].<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x : <span class="hljs-built_in">len</span>(re.findall(rx, x)) + <span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(df.mean())<br><span class="hljs-built_in">print</span>(df.median())<br><span class="hljs-built_in">print</span>(df.<span class="hljs-built_in">min</span>())<br><span class="hljs-built_in">print</span>(df.<span class="hljs-built_in">max</span>())<br><span class="hljs-built_in">print</span>(df[df&gt;<span class="hljs-number">18</span>].count())<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-number">5.5016</span><br><span class="hljs-number">5.0</span><br><span class="hljs-number">1</span><br><span class="hljs-number">52</span><br><span class="hljs-number">35</span><br></code></pre></td></tr></table></figure><p>老规矩，看一下主代码先：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3750</span>):<br>    <span class="hljs-keyword">if</span> (i + <span class="hljs-number">1</span>) % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>        <span class="hljs-built_in">print</span>(i)<br>    text_ = data[<span class="hljs-string">&#x27;full_text&#x27;</span>][i].strip()<br>    paragraph_count = get_paragraph_count(text_) <span class="hljs-comment">#计算段落数</span><br>    sen_count = get_sen_count(text_) <span class="hljs-comment">#计算句子数</span><br>    text = text_<br>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>): <span class="hljs-comment">#一个数据增强3倍</span><br>        <span class="hljs-keyword">if</span> paragraph_count &gt;= <span class="hljs-number">11</span>:<br>            text = get_seg_para(text_, j) <span class="hljs-comment">#对于段落数大于11的，按段落数来切</span><br>        <span class="hljs-keyword">elif</span> sen_count &gt;= <span class="hljs-number">18</span>:<br>            text = get_random_sen(text_) <span class="hljs-comment">#否则如果句子数大于18，随机取里面的句子，否则不做处理</span><br><br>        li = text.split(<span class="hljs-string">&quot; &quot;</span>) <span class="hljs-comment">#简单的分词</span><br>        length = <span class="hljs-built_in">len</span>(li)<br>        <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(length // <span class="hljs-number">10</span>): <span class="hljs-comment">#对于十分之一的词来操作</span><br><br>            <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>                index = random.choice(<span class="hljs-built_in">range</span>(length))<br>                pos = nltk.pos_tag([li[index]])[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>] <span class="hljs-comment">#分析词性，只操作名词和动词</span><br>                <span class="hljs-keyword">if</span> pos[<span class="hljs-number">0</span>] <span class="hljs-keyword">in</span> [<span class="hljs-string">&quot;V&quot;</span>, <span class="hljs-string">&quot;N&quot;</span>]:<br>                    p = random.uniform(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>) <span class="hljs-comment">#轮盘赌</span><br>                    <span class="hljs-keyword">if</span> p &gt; <span class="hljs-number">0.5</span>: <span class="hljs-comment">#50%概率不做改变</span><br>                        <span class="hljs-keyword">pass</span><br>                    <span class="hljs-keyword">elif</span> p &gt; <span class="hljs-number">0.3</span>: <span class="hljs-comment">#20%概率用编辑距离为1的破坏</span><br>                        li[index] = random.choice(edits1(li[index]))<br>                    <span class="hljs-keyword">else</span>:<br>                        <span class="hljs-keyword">try</span>:<br>                            li[index] = get_synonyms(li[index]) <span class="hljs-comment">#30%概率用同义词破替换</span><br>                        <span class="hljs-keyword">except</span> Exception:<br>                            li[index] = random.choice(edits1(li[index])) <span class="hljs-comment">#当然如果没有同义词，破坏之</span><br>                    <span class="hljs-keyword">break</span><br>        text = <span class="hljs-string">&quot; &quot;</span>.join(li)<br>        data.loc[<span class="hljs-built_in">len</span>(data)] = data.loc[i]<br>        data[<span class="hljs-string">&#x27;full_text&#x27;</span>][<span class="hljs-built_in">len</span>(data)-<span class="hljs-number">1</span>] = text<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Finish.&quot;</span>)<br></code></pre></td></tr></table></figure><p>各函数如下（编辑距离为1的函数不表，前文已经提到）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> random<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_paragraph_count</span>(<span class="hljs-params">txt</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(re.findall(<span class="hljs-string">&quot;\n\n&quot;</span>, txt)) + <span class="hljs-number">1</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_seg_para</span>(<span class="hljs-params">txt, index</span>):<br>    li = txt.split(<span class="hljs-string">&quot;\n\n&quot;</span>)<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(li) &lt;= <span class="hljs-number">18</span>: <span class="hljs-comment">#段落数小于等于18，大于等于11的，用大小为5的滑动窗口，比较平均</span><br>        seg_size = (<span class="hljs-built_in">len</span>(li) - <span class="hljs-number">5</span>) // <span class="hljs-number">3</span> <span class="hljs-comment">#计算步长</span><br>        delta = <span class="hljs-built_in">len</span>(li) - <span class="hljs-number">3</span> * seg_size - <span class="hljs-number">5</span> <span class="hljs-keyword">if</span> index == <span class="hljs-number">3</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span> <span class="hljs-comment">#保证填满</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;\n\n&quot;</span>.join(li[seg_size * index + delta : seg_size * index + <span class="hljs-number">5</span> + delta])<br>    <span class="hljs-keyword">else</span>: <span class="hljs-comment">#段落数过多了，就直接切分了，不用滑动窗口了</span><br>        seg_size = <span class="hljs-built_in">len</span>(li) // <span class="hljs-number">4</span><br>        delta = <span class="hljs-built_in">len</span>(li) % <span class="hljs-number">4</span> <span class="hljs-keyword">if</span> index == <span class="hljs-number">3</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;\n\n&quot;</span>.join(li[seg_size * index : seg_size * (index + <span class="hljs-number">1</span>) + delta])<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_sen_count</span>(<span class="hljs-params">txt</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(re.findall(<span class="hljs-string">r&#x27;[.!?]&#x27;</span>, txt))<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_random_sen</span>(<span class="hljs-params">txt</span>):<br>    li = re.split(<span class="hljs-string">&#x27;[.!?]&#x27;</span>, txt)<br>    l = <span class="hljs-built_in">len</span>(li)<br>    a = <span class="hljs-number">0</span><br>    b = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">while</span> b - a &lt;= l // <span class="hljs-number">4</span>: <span class="hljs-comment">#随机取一个区间，但要保证取出来的句子数至少是原来的1/4</span><br>        a = random.randint(<span class="hljs-number">0</span>, l)<br>        b = random.randint(<span class="hljs-number">0</span>, l)<br>        <span class="hljs-keyword">if</span> a &gt; b:<br>             a, b = b, a<br>    <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;. &quot;</span>.join(li[a:b])<br></code></pre></td></tr></table></figure><p>同义词（nltk这个库真的有点好用，虽然老是得手动下载一些语料库）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> nltk<br><span class="hljs-keyword">from</span> nltk.corpus <span class="hljs-keyword">import</span> wordnet<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_synonyms</span>(<span class="hljs-params">word</span>):<br>    synonyms = []<br>    <span class="hljs-keyword">for</span> syn <span class="hljs-keyword">in</span> wordnet.synsets(word):<br>        synonyms.extend(lm.name() <span class="hljs-keyword">for</span> lm <span class="hljs-keyword">in</span> syn.lemmas())<br>    <span class="hljs-keyword">return</span> random.choice(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(synonyms))) <span class="hljs-comment">#随机取同义词中的一个</span><br></code></pre></td></tr></table></figure><h2 id="实验效果的对比">实验效果的对比</h2><p>首先，先来看一下之前六个维度训练时，cohesion维度的效果：</p><p><img src="/img/article_content/2022-09-12/3.png" alt=""></p><p>可以看到0.51476604，这个维度拖后腿比较严重（也验证了我说相互牵制的预测，cohesion比较看重上下文）。</p><p>然后roberta-base单独训练，不使用增强数据时，降到了0.50多（忘了截图了），还是挺好的</p><p>使用增强数据后，又到了0.5-了：</p><p><img src="/img/article_content/2022-09-12/4.PNG" alt=""></p><p>然后我还用deberta-base+增强数据训练了一下，不得不说deberta确实是yyds哈，这下已经到了0.472了（截至发文还没训练完）：<img src="/img/article_content/2022-09-12/5.png" alt=""></p><p>四个点的增强还是很舒服的。</p>]]></content>
    
    
    <categories>
      
      <category>Kaggle实战</category>
      
      <category>English Language Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kaggle首战记录(4)-English Language Learning-baseline的数据增强</title>
    <link href="/2022/09/12/Kaggle%E9%A6%96%E6%88%98%E8%AE%B0%E5%BD%95(4)-English%20Language%20Learning-baseline%E7%9A%84%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/"/>
    <url>/2022/09/12/Kaggle%E9%A6%96%E6%88%98%E8%AE%B0%E5%BD%95(4)-English%20Language%20Learning-baseline%E7%9A%84%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/</url>
    
    <content type="html"><![CDATA[<p>数据增强后是否真的有用还有待商榷，但确实增加了0.01个点。另外还学会了nlp处理的很多库。</p><span id="more"></span><h2 id="前言">前言</h2><p>个人认为数据增强有几个要点：1.真的增强了；2.不要改变数据分布。</p><p>数据增强一般是在自变量做手脚，也就是说评分不变，如何去改输入。</p><p>NLP数据增强可以看看这篇：<a href="https://www.jianshu.com/p/bd7c9dad3100">NLP中简单的数据增强方法 - 简书 (jianshu.com)</a>，文中提到数据增强要做到：</p><p>（1）增加的数据要保证和原数据一致的语义信息。<br>（2）增加的数据需要多样化。</p><p>本题是长文本，数据增强其实有点困难，增删改词都行，但是变化少了——没多大用，变化大了——会影响评分。而且我们baseline是六个维度在一起的，适合词级别的增强不一定适合段落级别。并且，NLP那些回译、同义词等的改变，比较适合情感分类这种比较宏观的，不太适合这种需要注意到微观的文章评分。</p><p>因此我的解决办法如下：</p><p>1.在句号、逗号后随机增删空格、换行符（这对tokenizer有影响，算是一种噪声）</p><p>2.查找错误单词，改正一半的单词，并把同样数量的正确单词破坏</p><p>3.在to_tensor的时候，对于大于等于三个子句的文本，以大小为2的滑动窗口，1为步长滑动。</p><h2 id="代码">代码</h2><p>对前3700个数据做数据增强，剩下的是验证集（切忌全部增强再划分，否则验证集和训练集有相似的，很难公正地检测性能）</p><h3 id="主代码">主代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3700</span>):<br>    text_ = data[<span class="hljs-string">&#x27;full_text&#x27;</span>][i]<br>    err_list = get_error_words(text_)  <span class="hljs-comment">#得到文本中的错误单词，和改正后的单词，当然这种检测是纯传统的、统计式的</span><br><br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>): <span class="hljs-comment">#一个数据生成两个增强数据</span><br>        text = text_<br>        rx = <span class="hljs-string">r&quot;\.(?=\S)&quot;</span><br>        text = re.sub(rx, <span class="hljs-string">&quot;. &quot;</span>, text)<br>        rx = <span class="hljs-string">r&quot;\,(?=\S)&quot;</span><br>        text = re.sub(rx, <span class="hljs-string">&quot;, &quot;</span>, text) <span class="hljs-comment">#先把逗号，句号后的空格这些规范好</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(err_list) &gt; <span class="hljs-number">1</span>:<br>            choice_num = <span class="hljs-built_in">int</span>(p * (<span class="hljs-built_in">len</span>(err_list) + <span class="hljs-number">1</span>)) <span class="hljs-comment">#p设置为0.5</span><br>            need_to_correct = random.sample(err_list, choice_num) <span class="hljs-comment">#随机选择一半</span><br>            <span class="hljs-keyword">for</span> word_tuple <span class="hljs-keyword">in</span> need_to_correct:<br>                text = text.replace(word_tuple[<span class="hljs-number">0</span>], word_tuple[<span class="hljs-number">1</span>], <span class="hljs-number">1</span>) <span class="hljs-comment">#改正</span><br>            spl = <span class="hljs-built_in">list</span>(text.split()) <span class="hljs-comment">#以空格分词</span><br>            d = random.sample(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(spl)), choice_num) <span class="hljs-comment">#随机取文中的单词破坏</span><br>            <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> d:<br>                choosepool = <span class="hljs-built_in">list</span>(edits1(spl[k])) <span class="hljs-comment">#edits1为编辑距离为1的单词</span><br>                spl[k] = random.choice(choosepool)<br>            text = <span class="hljs-string">&#x27; &#x27;</span>.join(spl)<br>        <br>        text = correct_text(text) <span class="hljs-comment">#随机加空格和换行</span><br>        data.loc[<span class="hljs-built_in">len</span>(data)] = data.loc[i]<br>        data[<span class="hljs-string">&#x27;full_text&#x27;</span>][<span class="hljs-built_in">len</span>(data)-<span class="hljs-number">1</span>] = text<br>    <span class="hljs-keyword">if</span> i % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:<br>        <span class="hljs-built_in">print</span>(i)<br></code></pre></td></tr></table></figure><h3 id="各函数实现">各函数实现</h3><p>导入这个库，检查单词是否正确的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> enchant.checker <span class="hljs-keyword">import</span> SpellChecker<br></code></pre></td></tr></table></figure><p>先来看编辑距离为1的函数，这是魔改网上看到的一个函数得到的。我自己加了一个0.5编辑距离，也就是有单引号和没单引号之间的距离视为更小，因为Cheker库得到的修改单词不太智能，是<strong>返回一个列表</strong>，我需要自己在这个列表中选择更好的单词来修改。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">edits1</span>(<span class="hljs-params">word, typ = <span class="hljs-string">&quot;nohalf&quot;</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        返回跟输入单词是1距离的单词</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 26个英文字母   ord():获取&#x27;a&#x27;的码  chr():通过码还原对应的字符</span><br>    t = [<span class="hljs-built_in">chr</span>(<span class="hljs-built_in">ord</span>(<span class="hljs-string">&#x27;a&#x27;</span>) + i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">26</span>)]<br>    t.extend([<span class="hljs-built_in">chr</span>(<span class="hljs-built_in">ord</span>(<span class="hljs-string">&#x27;A&#x27;</span>) + i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">26</span>)])<br>    alphabet = <span class="hljs-string">&#x27;&#x27;</span>.join(t)<br>    <span class="hljs-keyword">if</span> typ == <span class="hljs-string">&#x27;half&#x27;</span>:<br>        alphabet = <span class="hljs-string">&#x27;&#x27;</span>.join([<span class="hljs-built_in">chr</span>(<span class="hljs-built_in">ord</span>(<span class="hljs-string">&quot;&#x27;&quot;</span>))])<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">splits</span>(<span class="hljs-params">word</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">            分割单词    以cta为例：  (&quot;&quot;,&quot;cat&quot;)  (&quot;c&quot;,&quot;at&quot;)  (&quot;ca&quot;,&quot;&quot;)   (&quot;cat&quot;,&quot;&quot;)</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> [(word[:i], word[i:])<br>                <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(word) + <span class="hljs-number">1</span>)]<br><br>    <span class="hljs-comment"># 分割好的单词</span><br>    pairs = splits(word)<br><br>    <br>    deletes = []<br>    transposes = []<br><br>    <span class="hljs-keyword">if</span> typ != <span class="hljs-string">&#x27;half&#x27;</span>:<br>        <span class="hljs-comment"># 删除某个字符</span><br>        deletes = [a + b[<span class="hljs-number">1</span>:] <span class="hljs-keyword">for</span> (a, b) <span class="hljs-keyword">in</span> pairs <span class="hljs-keyword">if</span> b]<br>        <span class="hljs-comment"># 两个字符换位置</span><br>        transposes = [a + b[<span class="hljs-number">1</span>] + b[<span class="hljs-number">0</span>] + b[<span class="hljs-number">2</span>:] <span class="hljs-keyword">for</span> (a, b) <span class="hljs-keyword">in</span> pairs <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(b) &gt; <span class="hljs-number">1</span>]<br>    <span class="hljs-comment"># 替换某个字符</span><br>    replaces = [a + c + b[<span class="hljs-number">1</span>:] <span class="hljs-keyword">for</span> (a, b) <span class="hljs-keyword">in</span> pairs <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> alphabet <span class="hljs-keyword">if</span> b]<br>    <span class="hljs-comment"># 插入某个字符</span><br>    inserts = [a + c + b <span class="hljs-keyword">for</span> (a, b) <span class="hljs-keyword">in</span> pairs <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> alphabet]<br>    <span class="hljs-comment"># 返回集合</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">set</span>(deletes + transposes + replaces + inserts)<br></code></pre></td></tr></table></figure><p>这里更倾向于选择编辑距离为1的单词，如果没有再随便选。而如果有相差一个单引号的更好。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> contextlib<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_error_words</span>(<span class="hljs-params">sen</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    返回错误单词的二元组的列表：(错误, 正确)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    chkr = SpellChecker(<span class="hljs-string">&quot;en_US&quot;</span>) <span class="hljs-comment">#引入语料库</span><br>    chkr.set_text(sen) <span class="hljs-comment">#检查单词</span><br>    err_list = []<br>    <span class="hljs-keyword">for</span> err <span class="hljs-keyword">in</span> chkr:<br>        correct_list = chkr.suggest(err.word)<br>        a = edits1(err.word, <span class="hljs-string">&#x27;half&#x27;</span>) <span class="hljs-comment">#相差一个单引号</span><br>        b = edits1(err.word) <span class="hljs-comment">#编辑距离为1</span><br>        edits1_list = []<br>        perfect = <span class="hljs-literal">False</span><br>        <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> correct_list:<br>            <span class="hljs-keyword">if</span> word <span class="hljs-keyword">in</span> a:<br>                err_list.append((err.word, word))<br>                perfect = <span class="hljs-literal">True</span><br>                <span class="hljs-keyword">break</span><br>            <span class="hljs-keyword">if</span> word <span class="hljs-keyword">in</span> b:<br>                edits1_list.append(word)<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> perfect:<br>            <span class="hljs-keyword">if</span> edits1_list:<br>                err_list.append((err.word, edits1_list[<span class="hljs-number">0</span>]))<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-keyword">with</span> contextlib.suppress(Exception):<br>                    err_list.append((err.word, correct_list[<span class="hljs-number">0</span>]))<br>    <span class="hljs-keyword">return</span> err_list<br><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">correct</span>(<span class="hljs-params">word</span>):<br>    a = random.uniform(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">if</span> a &lt;= <span class="hljs-number">0.15</span>:<br>        <span class="hljs-keyword">return</span> word[<span class="hljs-number">0</span>] <span class="hljs-comment">#15%的概率消除空格</span><br>    <span class="hljs-keyword">elif</span> <span class="hljs-number">0.15</span> &lt; a &lt;= <span class="hljs-number">0.25</span>:<br>        <span class="hljs-keyword">return</span> word + <span class="hljs-string">&#x27;\n\n&#x27;</span> <span class="hljs-comment">#10%的概率增加换行</span><br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> word <span class="hljs-comment">#其余不变</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">correct_match</span>(<span class="hljs-params"><span class="hljs-keyword">match</span></span>):<br>    word = <span class="hljs-keyword">match</span>.group()<br>    <span class="hljs-keyword">return</span> correct(word)<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">correct_text</span>(<span class="hljs-params">text</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    对符号后的空格进行是否删除</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> re.sub(<span class="hljs-string">&#x27;[,.?!] &#x27;</span>, correct_match, text)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">data.to_csv(<span class="hljs-string">&quot;newtrain.csv&quot;</span>, index=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure><p>最后再储存起来，得到了11000条数据。</p><h2 id="结果">结果</h2><p>训练效果是有提升的，原来的baseline训练的时候也最高到0.48。</p><p><img src="/img/article_content/2022-09-12/0.png" alt=""></p><p>在CPU上也达到第6名（比前几天高我的两位哥们高了，但是杀出来几个更牛的）</p><p><img src="/img/article_content/2022-09-12/1.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>Kaggle实战</category>
      
      <category>English Language Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kaggle首战记录(3)-English Language Learning-baseline的设计和训练</title>
    <link href="/2022/09/11/Kaggle%E9%A6%96%E6%88%98%E8%AE%B0%E5%BD%95(3)-English%20Language%20Learning-baseline%E7%9A%84%E8%AE%BE%E8%AE%A1%E5%92%8C%E8%AE%AD%E7%BB%83/"/>
    <url>/2022/09/11/Kaggle%E9%A6%96%E6%88%98%E8%AE%B0%E5%BD%95(3)-English%20Language%20Learning-baseline%E7%9A%84%E8%AE%BE%E8%AE%A1%E5%92%8C%E8%AE%AD%E7%BB%83/</url>
    
    <content type="html"><![CDATA[<h1>Kaggle首战记录(3)-English Language Learning-baseline的设计和训练</h1><p>一个模型训练效果的好坏除了模型本身，还很依赖于训练资源——数据和CPU</p><span id="more"></span><h2 id="baseline的设计和代码">baseline的设计和代码</h2><p>基于上述原因的权衡，baseline采取roberta-base + 两层全连接层的模式。采用roberta最后一层的隐藏层的第一个向量（也就是CLS的embedding），经过全连接层——batchnorm层——relu层——全连接层——sigmoid到(0, 6)作为输出。损失函数采用MSE。</p><p>batchnorm是代替dropout的正则化方法，但用在此是<strong>有疑问的</strong>。经过测试，roberta大概占1700MB显存，使用AdamW优化器的情况下，结合上一篇文章的数据处理方法（一个句子最多5个子句，说明一个batch的向量数最多是batchsize * 5），刚好能支撑batchsize = 4的情况（这就是不选deberta的原因——参数量大、训练慢，而且batchsize更小）。但是batchnorm对小批量的影响肯定会比较差。</p><p>初始化部分的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python">path = <span class="hljs-string">&#x27;../input/feedback-prize-english-language-learning/train.csv&#x27;</span><br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br>data = pd.read_csv(path)<br>data[<span class="hljs-string">&#x27;full_text&#x27;</span>] = data[<span class="hljs-string">&#x27;full_text&#x27;</span>].apply(<span class="hljs-keyword">lambda</span> x: x.strip())<br><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> random<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">init_seeds</span>(<span class="hljs-params">seed=<span class="hljs-number">7</span></span>):<br>    random.seed(seed)  <span class="hljs-comment"># seed for module random</span><br>    np.random.seed(seed)  <span class="hljs-comment"># seed for numpy</span><br>    torch.manual_seed(seed)  <span class="hljs-comment"># seed for PyTorch CPU</span><br>    torch.cuda.manual_seed(seed)  <span class="hljs-comment"># seed for current PyTorch GPU</span><br>    torch.cuda.manual_seed_all(seed)  <span class="hljs-comment"># seed for all PyTorch GPUs</span><br>    <span class="hljs-keyword">if</span> seed == <span class="hljs-number">0</span>:<br>        <span class="hljs-comment"># if True, causes cuDNN to only use deterministic convolution algorithms. </span><br>        torch.backends.cudnn.deterministic = <span class="hljs-literal">True</span><br>        <span class="hljs-comment"># if True, causes cuDNN to benchmark multiple convolution algorithms and select the fastest.</span><br>        torch.backends.cudnn.benchmark = <span class="hljs-literal">False</span><br>        <br>device = <span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span><br><br>init_seeds(<span class="hljs-number">42</span>)<br><br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RobertaTokenizer, RobertaModel<br>tokenizer = RobertaTokenizer.from_pretrained(<span class="hljs-string">&#x27;../input/roberta-base&#x27;</span>)<br>model = RobertaModel.from_pretrained(<span class="hljs-string">&#x27;../input/roberta-base&#x27;</span>).to(device)<br></code></pre></td></tr></table></figure><h3 id="网络设计">网络设计</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Class_Pool_Net</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, batch_size, pretrained_model, device</span>):<br>        <span class="hljs-built_in">super</span>(Class_Pool_Net, self).__init__()<br>        self.batch_size = batch_size<br>        self.model = pretrained_model<br>        self.linear1 = nn.Linear(<span class="hljs-number">768</span>, <span class="hljs-number">256</span>).to(device)<br>        self.batchnorm = nn.BatchNorm1d(<span class="hljs-number">256</span>).to(device)<br>        self.linear2 = nn.Linear(<span class="hljs-number">256</span>, <span class="hljs-number">6</span>).to(device)<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        output_embedding = self.model(x)[<span class="hljs-string">&#x27;last_hidden_state&#x27;</span>][:,<span class="hljs-number">0</span>].reshape([self.batch_size, -<span class="hljs-number">1</span>, <span class="hljs-number">768</span>])<br>        y1 = F.adaptive_max_pool2d(<span class="hljs-built_in">input</span>=output_embedding, output_size=(<span class="hljs-number">1</span>, <span class="hljs-number">768</span>)).squeeze(<span class="hljs-number">1</span>) <span class="hljs-comment">#对embedding出来先做maxpool</span><br>        y2 = self.linear1(y1)<br>        y3 = self.batchnorm(y2)<br>        y4 = F.relu(y3)<br>        y5 = self.linear2(y4)<br>        y6 = torch.sigmoid(y5) * <span class="hljs-number">6</span> <span class="hljs-comment">#映射到[0, 6]</span><br>        <span class="hljs-keyword">return</span> y6<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">change_batch_size</span>(<span class="hljs-params">self, size</span>):<br>        self.batch_size = size<br></code></pre></td></tr></table></figure><h3 id="评价函数">评价函数</h3><p>评价函数小抄了一手别人的代码，因为刚开始自己一直理解错了，以为是横向的mse，而且也没加根号，导致自己结果一直很难看，后来没想到是纵向的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> mean_squared_error<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate_function</span>(<span class="hljs-params">y_preds, y_trues</span>):<br>    scores = []<br>    y_preds = y_preds.cpu()<br>    y_trues = y_trues.cpu()<br>    idxes = y_trues.shape[<span class="hljs-number">1</span>]<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(idxes):<br>        y_true = y_trues[:,i]<br>        y_pred = y_preds[:,i]<br>        score = mean_squared_error(y_true, y_pred, squared=<span class="hljs-literal">False</span>) <span class="hljs-comment"># RMSE</span><br>        scores.append(score)<br>    mcrmse_score = np.mean(scores)<br>    <span class="hljs-keyword">return</span> mcrmse_score<br></code></pre></td></tr></table></figure><h3 id="训练函数">训练函数</h3><h4 id="学习率的设置">学习率的设置</h4><p>这里有一个坑，刚开始的时候，我把预训练层和全连接层的学习率都设为1e-5，效果非常差。后来看网上文章，才发现这两个学习率不用必须一样，未训练的层肯定要有更大的学习率。下图的函数就是把学习率精确到每一层。对于bias和Norm层不要权重衰减，全连接层的学习率调大到1e-2。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_group_parameters</span>(<span class="hljs-params">model</span>):<br>    params = <span class="hljs-built_in">list</span>(model.named_parameters())<br>    no_decay = [<span class="hljs-string">&#x27;bias&#x27;</span>,<span class="hljs-string">&#x27;LayerNorm&#x27;</span>, <span class="hljs-string">&#x27;batchnorm&#x27;</span>]<br>    other = [<span class="hljs-string">&#x27;linear1&#x27;</span>, <span class="hljs-string">&#x27;linear2&#x27;</span>]<br>    no_main = no_decay + other<br><br>    param_group = [<br>        &#123;<span class="hljs-string">&#x27;params&#x27;</span>:[p <span class="hljs-keyword">for</span> n,p <span class="hljs-keyword">in</span> params <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">any</span>(nd <span class="hljs-keyword">in</span> n <span class="hljs-keyword">for</span> nd <span class="hljs-keyword">in</span> no_main)],<span class="hljs-string">&#x27;weight_decay&#x27;</span>:<span class="hljs-number">1e-2</span>,<span class="hljs-string">&#x27;lr&#x27;</span>:<span class="hljs-number">1e-5</span>&#125;,<br>        &#123;<span class="hljs-string">&#x27;params&#x27;</span>:[p <span class="hljs-keyword">for</span> n,p <span class="hljs-keyword">in</span> params <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">any</span>(nd <span class="hljs-keyword">in</span> n <span class="hljs-keyword">for</span> nd <span class="hljs-keyword">in</span> other) <span class="hljs-keyword">and</span> <span class="hljs-built_in">any</span>(nd <span class="hljs-keyword">in</span> n <span class="hljs-keyword">for</span> nd <span class="hljs-keyword">in</span> no_decay) ],<span class="hljs-string">&#x27;weight_decay&#x27;</span>:<span class="hljs-number">0</span>,<span class="hljs-string">&#x27;lr&#x27;</span>:<span class="hljs-number">1e-5</span>&#125;,<br>        &#123;<span class="hljs-string">&#x27;params&#x27;</span>:[p <span class="hljs-keyword">for</span> n,p <span class="hljs-keyword">in</span> params <span class="hljs-keyword">if</span> <span class="hljs-built_in">any</span>(nd <span class="hljs-keyword">in</span> n <span class="hljs-keyword">for</span> nd <span class="hljs-keyword">in</span> other) <span class="hljs-keyword">and</span> <span class="hljs-built_in">any</span>(nd <span class="hljs-keyword">in</span> n <span class="hljs-keyword">for</span> nd <span class="hljs-keyword">in</span> no_decay) ],<span class="hljs-string">&#x27;weight_decay&#x27;</span>:<span class="hljs-number">0</span>,<span class="hljs-string">&#x27;lr&#x27;</span>:<span class="hljs-number">1e-2</span>&#125;,<br>        &#123;<span class="hljs-string">&#x27;params&#x27;</span>:[p <span class="hljs-keyword">for</span> n,p <span class="hljs-keyword">in</span> params <span class="hljs-keyword">if</span> <span class="hljs-built_in">any</span>(nd <span class="hljs-keyword">in</span> n <span class="hljs-keyword">for</span> nd <span class="hljs-keyword">in</span> other) <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">any</span>(nd <span class="hljs-keyword">in</span> n <span class="hljs-keyword">for</span> nd <span class="hljs-keyword">in</span> no_decay) ],<span class="hljs-string">&#x27;weight_decay&#x27;</span>:<span class="hljs-number">1e-2</span>,<span class="hljs-string">&#x27;lr&#x27;</span>:<span class="hljs-number">1e-2</span>&#125;,<br>    ]<br>    <span class="hljs-keyword">return</span> param_group<br></code></pre></td></tr></table></figure><h4 id="梯度累积">梯度累积</h4><p>这个名词之前也听过但不以为意。直到这次batch_size实在太小了（在实验室10GB的卡上，batchsize只能为2，而且还不能用AdamW，只能用SGD；在kaggle的16GB的卡上也只能batchsize=4）。batchsize小的缺点是，容易震荡，模型每次反向传播的时候只能学到小batch的东西，导致梯度下降的方向反复横跳。梯度累积就是为这个而生的，每次计算梯度，先不要反向传播，等到累积到一定步数再计算平均梯度再传播，效果相当于一个大的batchsize。当然梯度累积也不能完全解决小batch的问题，例如batchnorm在小batch上肯定效果不好。</p><h4 id="训练函数-2">训练函数</h4><p>以前的版本找不到了，用一个一折的版本</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><code class="hljs python">batch_size = <span class="hljs-number">4</span><br>epoch_num = <span class="hljs-number">10</span><br>lr = <span class="hljs-number">1e-5</span><br>loss = nn.MSELoss()<br><br>accumulate_steps = <span class="hljs-number">80</span><br><br>train_dataset = writing_dataset()<br>val_dataset = writing_dataset(data=data, typ=<span class="hljs-string">&#x27;val&#x27;</span>)<br><br>train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, collate_fn=collate, drop_last=<span class="hljs-literal">True</span>)<br>val_loader = DataLoader(dataset=val_dataset, batch_size=<span class="hljs-number">1</span>, collate_fn=collate)<br><br>net = Class_Pool_Net(batch_size, model, device)<br>param = get_group_parameters(net)<br>optimizer = torch.optim.AdamW(param, lr=lr)<br>scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">8</span>], gamma=<span class="hljs-number">0.4</span>) <span class="hljs-comment">#学习率优化，其实有人说用了AdamW就不用手动调了</span><br>lo = <span class="hljs-number">0</span><br>min_lo = <span class="hljs-number">2e5</span><br>min_mse = <span class="hljs-number">200</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epoch_num):<br>    lo = <span class="hljs-number">0</span><br>    net.train()<br>    net.batch_size=batch_size<br>    <span class="hljs-keyword">for</span> i, (X, y) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader):<br>        X = X.to(device)<br>        y = y.to(device)<br>        y_hat = net(X)<br>        l = loss(y_hat, y)<br>        l = l / accumulate_steps<br>        lo += l.item()<br>        l.backward() <span class="hljs-comment">#backward只是计算反向传播的梯度而已</span><br>        <span class="hljs-keyword">if</span> (i + <span class="hljs-number">1</span>) % accumulate_steps == <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> (i + <span class="hljs-number">1</span>) == <span class="hljs-built_in">len</span>(train_loader):<span class="hljs-comment">#梯度累积，step是把梯度作用到参数上</span><br>            optimizer.step()<br>            optimizer.zero_grad()<br>        <span class="hljs-keyword">if</span> (i + <span class="hljs-number">1</span>) % (accumulate_steps * <span class="hljs-number">8</span>) == <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> (i + <span class="hljs-number">1</span>) == <span class="hljs-built_in">len</span>(train_loader): <span class="hljs-comment">#这是在数据集很大且有大量的数据增强的时候用的，不用直到每个epoch算完再validation</span><br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;the <span class="hljs-subst">&#123;epoch&#125;</span>th: <span class="hljs-subst">&#123;<span class="hljs-number">100</span> * (i + <span class="hljs-number">1</span>) / <span class="hljs-built_in">len</span>(train_loader)&#125;</span> %&#x27;</span>)<br>            mse = <span class="hljs-number">0</span><br>            net.<span class="hljs-built_in">eval</span>()<br>            net.batch_size=<span class="hljs-number">1</span><br>            ypred = <span class="hljs-literal">None</span><br>            ytrue = <span class="hljs-literal">None</span><br>            <span class="hljs-keyword">for</span> i,(X, y) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(val_loader):<br>                <span class="hljs-keyword">with</span> torch.no_grad():<br>                    X = X.to(device)<br>                    y = y.to(device)<br>                    y_hat = net(X)<br>                    <span class="hljs-keyword">if</span> i == <span class="hljs-number">0</span>:<br>                        ypred = y_hat<br>                        ytrue = y<br>                    <span class="hljs-keyword">else</span>:<br>                        ypred = torch.cat((ypred, y_hat))<br>                        ytrue = torch.cat((ytrue, y))<br>            mse = evaluate_function(ypred, ytrue)<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;mse: <span class="hljs-subst">&#123;mse&#125;</span>.&#x27;</span>)<br>            <span class="hljs-keyword">if</span> mse &lt; min_mse:<br>                torch.save(&#123;<span class="hljs-string">&#x27;model&#x27;</span>: net.state_dict()&#125;, <span class="hljs-string">f&#x27;./minmse_fold.pth&#x27;</span>)<br>                min_mse = mse<br>            net.train()<br>            net.batch_size=batch_size<br>    scheduler.step()<br>   <br>    <span class="hljs-keyword">if</span> lo &lt; min_lo:<br>        torch.save(&#123;<span class="hljs-string">&#x27;model&#x27;</span>: net.state_dict()&#125;, <span class="hljs-string">f&#x27;./minloss_fold.pth&#x27;</span>)<br>        min_lo = lo<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;epoch&#125;</span>th epoch: last loss: <span class="hljs-subst">&#123;lo * accumulate_steps&#125;</span>.&#x27;</span>)<br>    <span class="hljs-keyword">if</span> mse &lt; min_mse:<br>        torch.save(&#123;<span class="hljs-string">&#x27;model&#x27;</span>: net.state_dict()&#125;, <span class="hljs-string">f&#x27;./minmse_fold.pth&#x27;</span>)<br>        min_mse = mse<br><span class="hljs-comment">#     print(f&#x27;&#123;epoch&#125;th epoch: last loss: &#123;lo * accumulate_steps&#125; and mse: &#123;mse&#125;.&#x27;)</span><br><br>torch.save(&#123;<span class="hljs-string">&#x27;model&#x27;</span>: net.state_dict()&#125;, <span class="hljs-string">f&#x27;./last_fold.pth&#x27;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;End.&#x27;</span>)<br></code></pre></td></tr></table></figure><h2 id="训练结果">训练结果</h2><p>原始数据已经遗失了，在26%测试集上是0.47的结果，在当天排200多名，现在已经掉到300多了。</p><p>然后0.47在CPU上的排名，最高排过前5哈哈。不知道这个赛道有没有发奖牌的呢？还是只有前三名有钱啊qaq。</p><p><img src="/img/article_content/2022-09-11/2.png" alt=""></p><h2 id="不足与展望">不足与展望</h2><h3 id="不足">不足</h3><p>1.数据少</p><p>2.六个维度一起训练，肯定不太可能达到很好</p><p>3.使用roberta。deberta-large我老是显存溢出，使用半精度训练效果很一般。</p><p>4.我还没空十折交叉训练，也没空跑几个不同的种子。</p><p>5.batchnorm到底有没有用也难说</p><h3 id="展望">展望</h3><p>CPU和GPU肯定要有不同的方向。</p><p>CPU的话，2肯定改进不了了，打算从数据增强入手。</p><p>GPU的话，应着重改善2。</p><p>可能可以先在GPU训练6个单独的比较好的网络，然后用伪标签训练来反哺CPU的网络。</p>]]></content>
    
    
    <categories>
      
      <category>Kaggle实战</category>
      
      <category>English Language Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kaggle首战记录(2)-English Language Learning-baseline的数据处理</title>
    <link href="/2022/09/11/Kaggle%E9%A6%96%E6%88%98%E8%AE%B0%E5%BD%95(2)-English%20Language%20Learning-baseline%E7%9A%84%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"/>
    <url>/2022/09/11/Kaggle%E9%A6%96%E6%88%98%E8%AE%B0%E5%BD%95(2)-English%20Language%20Learning-baseline%E7%9A%84%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/</url>
    
    <content type="html"><![CDATA[<p>这部分是和baseline有关的数据处理环节。</p><span id="more"></span><h2 id="EDA">EDA</h2><p>和统计一样，上来得先做一个数据探索EDA。</p><p><img src="/img/article_content/2022-09-11/0.png" alt=""></p><p>瞄一下数据，大概是成正态分布，这比较符合常识，因此可以说不太存在数据不平衡的现象。</p><p>注意这里的数据不平衡是指训练集和真实分布的差距，而不是score值的相互比较。</p><p>数据共3911行，说实话不多，因此也比较依赖之后的数据增强。</p><p>baseline打算用roberta-base做预训练层，roberta的预训练任务token数都是不超过512的，因此EDA也要关注数据过roberta的tokenizer后的情况。</p><p>导入数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">path = <span class="hljs-string">&#x27;../../../mydata/ka/ell/train.csv&#x27;</span><br><br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br>data = pd.read_csv(path)<br>data[<span class="hljs-string">&#x27;full_text&#x27;</span>] = data[<span class="hljs-string">&#x27;full_text&#x27;</span>].apply(<span class="hljs-keyword">lambda</span> x: x.strip()) <span class="hljs-comment">#简单地去除一下头尾</span><br><br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RobertaTokenizer<br>tokenizer = RobertaTokenizer.from_pretrained(<span class="hljs-string">&#x27;../../../mydata/roberta-base/&#x27;</span>)<br></code></pre></td></tr></table></figure><h3 id="token数">token数</h3><p>然后把文本列用tokenizer映射一下，关注token数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">token_len_df = data[<span class="hljs-string">&#x27;full_text&#x27;</span>].<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x : tokenizer(x, return_tensors=<span class="hljs-string">&#x27;pt&#x27;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>].shape[<span class="hljs-number">1</span>])<br><br><span class="hljs-built_in">print</span>(token_len_df.<span class="hljs-built_in">max</span>())<br><span class="hljs-built_in">print</span>(token_len_df.<span class="hljs-built_in">min</span>())<br><span class="hljs-built_in">print</span>(token_len_df.mean())<br><span class="hljs-built_in">print</span>(token_len_df.median())<br><span class="hljs-built_in">print</span>(token_len_df.count())<br><span class="hljs-built_in">print</span>(token_len_df[token_len_df &gt; <span class="hljs-number">512</span>].count())<br></code></pre></td></tr></table></figure><p>结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-number">1457</span><br><span class="hljs-number">28</span><br><span class="hljs-number">493.9312196369215</span><br><span class="hljs-number">463.0</span><br><span class="hljs-number">3911</span><br><span class="hljs-number">1555</span><br></code></pre></td></tr></table></figure><p>可以看到最长的已经到1457了，而且大于512的有接近一半了，因此考虑切割。</p><h3 id="段落数">段落数</h3><p>切割怎么切好呢？段落一般是文意的分割点，因此我们再EDA一下段落：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_paragraph_count</span>(<span class="hljs-params">x</span>): <span class="hljs-comment">#计算段落数量，其实后来想想直接用正则表达式对文本操作不是更方便（</span><br>    sen_tensor = tokenizer(x, return_tensors=<span class="hljs-string">&#x27;pt&#x27;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>][<span class="hljs-number">0</span>]<br>    count = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> sen_tensor:<br>        <span class="hljs-keyword">if</span> token.item() == <span class="hljs-number">50118</span>:<br>            count += <span class="hljs-number">1</span><br>    <span class="hljs-keyword">return</span> count / <span class="hljs-number">2</span> + <span class="hljs-number">1</span><br><br>paragraph_count_df = data[<span class="hljs-string">&#x27;full_text&#x27;</span>].<span class="hljs-built_in">map</span>(get_paragraph_count)<br><br><span class="hljs-built_in">print</span>(paragraph_count_df.<span class="hljs-built_in">max</span>())<br><span class="hljs-built_in">print</span>(paragraph_count_df.<span class="hljs-built_in">min</span>())<br><span class="hljs-built_in">print</span>(paragraph_count_df.mean())<br><span class="hljs-built_in">print</span>(paragraph_count_df.median())<br><span class="hljs-built_in">print</span>(paragraph_count_df[paragraph_count_df &gt; <span class="hljs-number">30</span>].count())<br></code></pre></td></tr></table></figure><p>结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-number">52.0</span><br><span class="hljs-number">1.0</span><br><span class="hljs-number">5.538097673229353</span><br><span class="hljs-number">5.0</span><br><span class="hljs-number">5</span><br></code></pre></td></tr></table></figure><p>也就是有的居然达到了52段！实在是丧心病狂，看一下原文：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(data[<span class="hljs-string">&#x27;full_text&#x27;</span>][paragraph_count_df.idxmax()])<br></code></pre></td></tr></table></figure><p>输出如下，只能说很会玩：</p><p><img src="/img/article_content/2022-09-11/1.png" alt=""></p><p>本来我还进行了段落中最大token数的探索，但现在感觉没必要，无论探索结果如何，你都不可能仅仅把段落数作为唯一切割依据。</p><h2 id="数据处理">数据处理</h2><p>数据处理的方法就呼之欲出了，对于一条长文本，先转化为token，然后不断二分切割，当然这个二分只是“类”二分，最好按段落切割，其次的分割标准是句号。然后我发现有的作文连句号都没有，因此还加上了逗号和空格。</p><h3 id="处理过程">处理过程</h3><p>代码如下：</p><p>是用递归实现的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">txt_to_tensor</span>(<span class="hljs-params">x</span>):<br>    x = x.strip()<br>    sen_tensor = tokenizer(x, return_tensors=<span class="hljs-string">&#x27;pt&#x27;</span>, padding=<span class="hljs-string">&quot;max_length&quot;</span>, max_length=<span class="hljs-number">512</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>] <span class="hljs-comment">#padding补齐，不然没办法cat</span><br>    <span class="hljs-keyword">if</span> sen_tensor.shape[<span class="hljs-number">1</span>] &lt;= <span class="hljs-number">512</span>:  <span class="hljs-comment">#如果本段落合格了，则返回</span><br>        <span class="hljs-keyword">return</span> sen_tensor<br>    str_len_mid = <span class="hljs-built_in">len</span>(x) // <span class="hljs-number">2</span> <span class="hljs-comment">#句子中央</span><br>    about_mid = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">try</span>:<br>        about_mid = str_len_mid + x[str_len_mid:].index(<span class="hljs-string">&#x27;\n&#x27;</span>) <span class="hljs-comment">#否则尝试在句子中央之后找一找换行符</span><br>    <span class="hljs-keyword">except</span>:<br>        <span class="hljs-keyword">try</span>:<br>            about_mid = str_len_mid - x[str_len_mid::-<span class="hljs-number">1</span>].index(<span class="hljs-string">&#x27;\n&#x27;</span>) <span class="hljs-comment">#如果找不到，在句子中央之前找一找换行符</span><br>        <span class="hljs-keyword">except</span>:<br>            <span class="hljs-keyword">try</span>:<br>                about_mid = str_len_mid + x[str_len_mid:].index(<span class="hljs-string">&#x27;.&#x27;</span>) <span class="hljs-comment">#再找不到，找找句号</span><br>                x1 = txt_to_tensor(x[:about_mid])<br>                x2 = txt_to_tensor(x[about_mid+<span class="hljs-number">1</span>:])<br>                <span class="hljs-keyword">return</span> torch.cat((x1, x2))<br>            <span class="hljs-keyword">except</span>:<br>                about_mid = str_len_mid + x[str_len_mid:].index(<span class="hljs-string">&#x27;,&#x27;</span>) <span class="hljs-comment">#逗号</span><br>                x1 = txt_to_tensor(x[:about_mid])<br>                x2 = txt_to_tensor(x[about_mid+<span class="hljs-number">1</span>:])<br>                <span class="hljs-keyword">return</span> torch.cat((x1, x2))<br>    <span class="hljs-keyword">if</span> about_mid &gt; str_len_mid * <span class="hljs-number">1.5</span> <span class="hljs-keyword">or</span> about_mid &lt; str_len_mid * <span class="hljs-number">0.5</span>: <span class="hljs-comment">#尽量分割合理，如果两边长度差别太大，试试用句号分割</span><br>        <span class="hljs-keyword">try</span>:<br>            about_mid2 = str_len_mid + x[str_len_mid:].index(<span class="hljs-string">&#x27;.&#x27;</span>)<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">abs</span>(about_mid2 - str_len_mid) &lt; <span class="hljs-built_in">abs</span>(about_mid - str_len_mid):<br>                about_mid = about_mid2<br>        <span class="hljs-keyword">except</span>:<br>            <span class="hljs-keyword">pass</span><br>    x1 = txt_to_tensor(x[:about_mid]) <span class="hljs-comment">#再递归探索</span><br>    x2 = txt_to_tensor(x[about_mid+<span class="hljs-number">1</span>:])<br>    <span class="hljs-keyword">return</span> torch.cat((x1, x2))<br></code></pre></td></tr></table></figure><p>这个处理方法是<strong>有问题的</strong>：</p><p>首先，分割其实可以不用那么合理，毕竟连28个token的文章都有。</p><p>其次，对于不同的评分维度，数据处理的要求是不同的，本方法的目的是尽可能地保存“段落”、“章节”，而像语法句法、短语词汇这些可能根本不需要保留这些信息，但在baseline六个维度一起训练的条件下这还是有必要的。</p><p>处理方法的问题也给数据增强带来思路（虽然这不叫增强了叫查漏补缺）</p><p>不管那么多了，我们apply一下，再看看我们处理后每个数据的子句多少：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">data[<span class="hljs-string">&#x27;full_tensor&#x27;</span>] = data[<span class="hljs-string">&#x27;full_text&#x27;</span>].apply(txt_to_tensor)<br><br>d = data[<span class="hljs-string">&#x27;full_tensor&#x27;</span>].<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: x.shape[<span class="hljs-number">0</span>])<br><span class="hljs-built_in">print</span>(d.<span class="hljs-built_in">max</span>())<br><span class="hljs-built_in">print</span>(d[d&gt;<span class="hljs-number">1</span>].count())<br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-number">5</span><br><span class="hljs-number">1555</span><br></code></pre></td></tr></table></figure><p>也就是最多有5个子句，多于1个子句的有1555个，和上面大于512token的数量一样。</p><h3 id="数据集创建">数据集创建</h3><p>写一个十折交叉检验的数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">writing_dataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, data=data, ki = <span class="hljs-number">0</span>, typ=<span class="hljs-string">&#x27;train&#x27;</span></span>):<br>        self.x = []<br>        self.y = [] <span class="hljs-comment">#六维的评分</span><br>        self.k = <span class="hljs-number">10</span> <span class="hljs-comment">#10折交叉</span><br>        self.xy = []<br>        <span class="hljs-keyword">for</span> dat <span class="hljs-keyword">in</span> data.iterrows():<br>            self.x.append(dat[<span class="hljs-number">1</span>][<span class="hljs-string">&#x27;full_tensor&#x27;</span>])<br>            self.y.append(torch.tensor([dat[<span class="hljs-number">1</span>][<span class="hljs-string">&#x27;cohesion&#x27;</span>], dat[<span class="hljs-number">1</span>][<span class="hljs-string">&#x27;syntax&#x27;</span>], dat[<span class="hljs-number">1</span>][<span class="hljs-string">&#x27;vocabulary&#x27;</span>], dat[<span class="hljs-number">1</span>][<span class="hljs-string">&#x27;phraseology&#x27;</span>], dat[<span class="hljs-number">1</span>][<span class="hljs-string">&#x27;grammar&#x27;</span>], dat[<span class="hljs-number">1</span>][<span class="hljs-string">&#x27;conventions&#x27;</span>]]))<br>        self.length = <span class="hljs-built_in">len</span>(self.y)<br>        self.xy.extend((self.x[i], self.y[i]) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.length))<br>        random.seed(<span class="hljs-number">1</span>)<br>        random.shuffle(self.xy)<br>        self.my_xy = []<br>        every_z_len = self.length // self.k<br>        <span class="hljs-keyword">if</span> typ == <span class="hljs-string">&#x27;val&#x27;</span>:<br>            self.my_xy = self.xy[every_z_len * ki : every_z_len * (ki+<span class="hljs-number">1</span>)]<br>        <span class="hljs-keyword">elif</span> typ == <span class="hljs-string">&#x27;train&#x27;</span>:<br>            self.my_xy = self.xy[: every_z_len * ki] + self.xy[every_z_len * (ki+<span class="hljs-number">1</span>) :]<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Wrong type!&#x27;</span>)<br><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, index</span>):<br>        <span class="hljs-keyword">return</span> self.my_xy[index]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.my_xy)<br></code></pre></td></tr></table></figure><p>平平无奇的写法，注意数据集的自变量是二维的tensor，tensor的第一个维度（子句数）是不一样的，所以在dataloader的时候要写一个collate函数处理一下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">collate</span>(<span class="hljs-params">batches</span>):<br>    max_sub_count = <span class="hljs-number">0</span><br>    first = <span class="hljs-literal">True</span><br>    x = <span class="hljs-literal">None</span><br>    y = <span class="hljs-literal">None</span><br>    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> batches:<br>        max_sub_count = <span class="hljs-built_in">max</span>(max_sub_count, batch[<span class="hljs-number">0</span>].shape[<span class="hljs-number">0</span>]) <span class="hljs-comment">#统计batch内的最大子句数</span><br>    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> batches:<br>        <span class="hljs-keyword">if</span> first:<br>            first = <span class="hljs-literal">False</span><br>            x = batch[<span class="hljs-number">0</span>]<br>            y = batch[<span class="hljs-number">1</span>].unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-comment">#batch[1]的维度是[6]，这里变成了[1, 6]才好cat</span><br>        <span class="hljs-keyword">else</span>:<br>            x = torch.cat((x, batch[<span class="hljs-number">0</span>]))<br>            y = torch.cat((y, batch[<span class="hljs-number">1</span>].unsqueeze(<span class="hljs-number">0</span>)))<br><br>        need_sub_count = max_sub_count - batch[<span class="hljs-number">0</span>].shape[<span class="hljs-number">0</span>]<br><br>        <span class="hljs-keyword">if</span> need_sub_count:<br>            x = torch.cat((x, torch.repeat_interleave(batch[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>].unsqueeze(<span class="hljs-number">0</span>), need_sub_count, dim=<span class="hljs-number">0</span>))) <span class="hljs-comment">#没到最大子句的，取第一个子句来补充，注意，这导致了我们池化层必须是maxpool，或者反过来maxpool下我们才能这么做。</span><br><br>    <span class="hljs-keyword">return</span> x, y<br></code></pre></td></tr></table></figure><p>数据处理就到这里。</p>]]></content>
    
    
    <categories>
      
      <category>Kaggle实战</category>
      
      <category>English Language Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kaggle首战记录(1)-English Language Learning-比赛简介及读题</title>
    <link href="/2022/09/10/Kaggle%E9%A6%96%E6%88%98%E8%AE%B0%E5%BD%95(1)-English%20Language%20Learning-%E6%AF%94%E8%B5%9B%E7%AE%80%E4%BB%8B%E5%8F%8A%E8%AF%BB%E9%A2%98/"/>
    <url>/2022/09/10/Kaggle%E9%A6%96%E6%88%98%E8%AE%B0%E5%BD%95(1)-English%20Language%20Learning-%E6%AF%94%E8%B5%9B%E7%AE%80%E4%BB%8B%E5%8F%8A%E8%AF%BB%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<p>第一次参加kaggle的比赛，选择了这个比赛，现在距正式做已经一周时间了，发现做这个能让自己注意很多深度学习的细节（例如显存等平时上课看论文不太关心的），而且自己能在反思中收获很多东西。刚好一周 41hours 的GPU配额快用完了。因此决定把自己的比赛过程记录下来。</p><span id="more"></span><h2 id="比赛简介">比赛简介</h2><h3 id="时间">时间</h3><p><img src="/img/article_content/2022-09-10/3.png" alt=""></p><h3 id="内容">内容</h3><p>训练一个模型，评估 8-12 年级英语语言学习者 (ELL) 的语言能力，输入是每个学生的写作文本（长文本），输出是六个维度的评分，评分范围为1-5分，评分是离散的，0.5分为一个间隔。</p><p>需要评分的六个维度分别为：<span class="label label-primary">cohesion</span>  <span class="label label-primary">syntax</span> <span class="label label-primary">vocabulary</span> <span class="label label-primary">phraseology</span> <span class="label label-primary">grammar</span> <span class="label label-primary">conventions</span>。中文的直译是：连贯性、句法、词汇、短语、惯例用法（有一说一这些已经对非英语母语者不友好了qaq）。</p><p>评估模型的好坏有两个赛道，一个是质量赛道，一个是效率赛道。</p><p>质量赛道的评估函数为类MSE函数，其中N_t就是6，n是总测试样本数：</p><p><img src="/img/article_content/2022-09-10/4.png" alt=""></p><p>效率赛道在此基础上考虑运行时间（只允许在CPU上运行），函数为：</p><p><img src="/img/article_content/2022-09-10/5.png" alt=""></p><p>其中Base是baseline的MCRMSE，minMCRMSE是榜上的最好MCRMSE。</p><h2 id="读题">读题</h2><p>首次参加比赛还是有很多不懂的，这里只是抛砖引玉写自己的理解而已（毕竟现在在质量赛道还是排名很后的orz）。</p><p>首先，<strong>回归模型和分类模型</strong>是需要斟酌的，回归模型的好处是损失函数一般用MSE，和评估函数很接近；为什么考虑分类模型呢？这是因为评分是离散的，可以看成是分类，而且分类给人的感觉就比较容易达到高分。但是分类模型的损失函数——交叉熵及其变体，都不会考虑到<strong>分错的严重程度</strong>，比如1分的打成3分明显比打成5分好一点，我的初步思考是在生成标签的时候做一些手脚，比如标签平滑时对远处的标签赋为更小甚至负值（当然这样损失函数的计算要注意），但这样有没有理论依据，说实话我还是太菜了没想出来，所以采用了回归模型。</p><p>其次，这几个指标能不能一起训练？我的baseline（其实也是我目前的次优分数）就是直接在预训练模型后加两层linear——这导致了微调时预训练模型是同时要去适应六个维度的，细看六个维度，cohesion是<strong>paragraph-level</strong>的，syntax、grammar是<strong>sentence-level</strong>的，而vocabulary、conventions、phraseology虽然都是句子内部的，但侧重点又有所不同，所以同时训练结果必是比较差的。但是同时训练的好处是：它能在<strong>效率赛道</strong>上竞争！毕竟你让CPU跑一次大模型和跑两次大模型所用时间实在是天差地别。之后也会提到，在数据增强的时候，这几种的数据增强方向绝对是有不同的。</p><p>其他的后面想到再在后面的文章提了，或者在这里更新。</p>]]></content>
    
    
    <categories>
      
      <category>Kaggle实战</category>
      
      <category>English Language Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文阅读笔记 （LS导致隐式长度惩罚）</title>
    <link href="/2022/08/24/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%20%EF%BC%88LS%E5%AF%BC%E8%87%B4%E9%9A%90%E5%BC%8F%E9%95%BF%E5%BA%A6%E6%83%A9%E7%BD%9A%EF%BC%89/"/>
    <url>/2022/08/24/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%20%EF%BC%88LS%E5%AF%BC%E8%87%B4%E9%9A%90%E5%BC%8F%E9%95%BF%E5%BA%A6%E6%83%A9%E7%BD%9A%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<table><thead><tr><th><strong>题目</strong></th><th>The Implicit Length Bias of Label Smoothing on Beam Search Decoding</th></tr></thead><tbody><tr><td><strong>论文链接</strong></td><td><a href="https://arxiv.org/pdf/2205.00659.pdf">https://arxiv.org/pdf/2205.00659.pdf</a></td></tr><tr><td><strong>作者列表</strong></td><td>Bowen Liang, Pidong Wang, Yuan Cao</td></tr><tr><td><strong>作者单位</strong></td><td>Google</td></tr><tr><td><strong>文章类型</strong></td><td>短文</td></tr><tr><td><strong>撰写人</strong></td><td>C.Y.</td></tr></tbody></table><h2 id="核心任务和思想">核心任务和思想</h2><p>通过数学推导在理论上说明训练时采用标签平滑会隐式地导致beam search解码机器翻译任务时存在长度惩罚，使得解码倾向于给出较短的结果，且理论上，标签平滑的模型会使得解码的长度存在与输入无关的常数上界。作者在实验中也发现了上述现象，并提出了修正方法。</p><h2 id="背景知识">背景知识</h2><p>标签平滑是一种正则化防止过拟合的方法，它将独热的标签平滑为实数向量，计算方法如下：</p><p><img src="https://i1lqvqtgqs.feishu.cn/space/api/box/stream/download/asynccode/?code=ZjIzODc4ZjZjNzJmMTZmOTFmOWM5MmZhN2ZiMzA5ZWRfV2dEU1pQcGNGdWxVNXNjZk8yNVhYMmlNalVxcHdaMHBfVG9rZW46Ym94Y25sTktMRlFkSjdtV2tiaVBaOWlMUGtnXzE2NjEzNDg5NTQ6MTY2MTM1MjU1NF9WNA" alt="img"></p><p>其中q是平滑前的概率向量（向量的维度为词表大小V，向量中每个元素代表对应索引的词的概率），它是独热的。α是超参数，一般取为0.1。平滑后，原来为1的元素会略微减小，原来为0的元素会略微增大。</p><p>我们设模型给出的最后一层输出为向量q<code>，在充分训练后，q</code>可以视为模型给出的下一个时间步每个单词的概率，即有：</p><p><img src="https://i1lqvqtgqs.feishu.cn/space/api/box/stream/download/asynccode/?code=NmQ3ZjI1OWYyMjRkNDQzNDMyYTI0M2IxYWJlMzQyNzJfTHFYNDVBNDhmV1cxYTk4cXRRNkdMSXJUbkp5MzhlODNfVG9rZW46Ym94Y25QRXA2VVlIV2pXWVA0SlF5OHQ3cm1jXzE2NjEzNDg5NTQ6MTY2MTM1MjU1NF9WNA" alt="img"></p><p>在beam search解码时，我们的目标是得到一个对数概率尽量大的序列，但由上文可知，我们使用的p_hat和真正的概率q存在一定偏差，因此有下式：</p><p><img src="https://i1lqvqtgqs.feishu.cn/space/api/box/stream/download/asynccode/?code=M2M0ZDBhZjViMTZmZGNiY2I2OGI1ODk4NjEyZTdhYWNfeEh3M0h1a3JJWFdrSnVrRzRsRXB5U1JxZmFyWndZYkxfVG9rZW46Ym94Y25TUHFDWW53VVk3emtxNkJhREFON1dpXzE2NjEzNDg5NTQ6MTY2MTM1MjU1NF9WNA" alt="img"></p><p>其中T是序列长度，y_t是t时间步的输出（序列的第t个词），可以看到最后序列的对数概率与真实的对数概率存在与序列长度线性相关的偏差，而log(1-α)是一个负值，当α取0.1时为-0.105，导致T越小，序列越短，对数概率越大，因此解码倾向于给出越短的序列。</p><p>继续计算，我们可以得到p_hat的上下界：</p><p><img src="https://i1lqvqtgqs.feishu.cn/space/api/box/stream/download/asynccode/?code=YjdiN2Q5M2M2ZjJkMzhjNGVmMDllMDdiYTJhNTY1NWZfNkxIaU5PWTJmOUU4eWhIVzVpRDRXcTJlNWdsdEdDaGRfVG9rZW46Ym94Y241UTVoSW82RVBraDBXZnFBZWRPZWdkXzE2NjEzNDg5NTQ6MTY2MTM1MjU1NF9WNA" alt="img"></p><p>那么对于空序列（仅由EOS构成）和某个序列，有：</p><p><img src="https://i1lqvqtgqs.feishu.cn/space/api/box/stream/download/asynccode/?code=ZjMzNjRiMTkwMmY2ZDc1NGI3ZmY4MWU4MmZmODlmNTNfTVJqUTVDRjRlRWZMWUNFWGozTTFJUTVTMDhOc1g5Z1VfVG9rZW46Ym94Y25MNlk4ZmVITmxDa3lEWjdFZFEwUmJqXzE2NjEzNDg5NTQ6MTY2MTM1MjU1NF9WNA" alt="img"></p><p>两式比较，可以得到T有如此上界：</p><p><img src="https://i1lqvqtgqs.feishu.cn/space/api/box/stream/download/asynccode/?code=NWQxNWJiZjZkYjYxYTc3YzM4ZjM4ZDM2MjQ5YjVjMTdfdnJDSkxxcHI5b3VRMnFaWW9rMTFaNnhmbjhNZHA0SXFfVG9rZW46Ym94Y245eDdqMFlGNDQxdElSMkQweEN4RGllXzE2NjEzNDg5NTQ6MTY2MTM1MjU1NF9WNA" alt="img"></p><h2 id="方法">方法</h2><p>通过上述的理论推导，将第一个公式逆过来，得到修正公式：</p><p><img src="https://i1lqvqtgqs.feishu.cn/space/api/box/stream/download/asynccode/?code=ZWUwMGJmNDk4YmM0MzlkMDliMzY4N2ExOTM3Yjc2MjZfN2Y4RmoyN2VRTUZtTDNEUkJ6RnN3WlFCNDNIOWIzdUxfVG9rZW46Ym94Y25UTHpMSEhsRWdGRERSODZpNE1PSkliXzE2NjEzNDg5NTQ6MTY2MTM1MjU1NF9WNA" alt="img"></p><p>由于模型训练的结果不完全是概率，这样计算后概率值可能会超出[0, 1]，因此作者又增加了ReLU来避免这种情况。</p><p>加上归一化，最终结果如下：</p><p><img src="https://i1lqvqtgqs.feishu.cn/space/api/box/stream/download/asynccode/?code=OGNjYjQ2MWIzNTY1YjI1YzE2ZmRkOTVjY2VkMDMxYjBfcHpzMzZrZzVyWGVtb1BtSXRjUm5odFdXSE9sRGw3RFBfVG9rZW46Ym94Y25pRzJURGRETjQ2czNHbTBXZjJwQ1J6XzE2NjEzNDg5NTQ6MTY2MTM1MjU1NF9WNA" alt="img"></p><p>其中δ=α / V 。</p><p>作者使用修正后的概率值，使用beam search进行实验。</p><p>作者将模型的 δ 设为  n / V，其中n = 0.1、1、100，同时也探究了beam size分别为1、4、8、25、100、200的结果。</p><h2 id="实验">实验</h2><ul><li><h3 id="数据集">数据集</h3></li><li><p>WMT19 EnDe, EnCs, EnZh 和 WMT15 EnFr.</p></li><li><h3 id="评测指标">评测指标</h3></li><li><p>BLEU值</p></li><li><p>翻译长度 / 源句子长度</p></li><li><h3 id="基线方法">基线方法</h3></li><li><p>基于Transformer训练的机器翻译模型，没有标签平滑。</p></li><li><p>基于Transformer训练的机器翻译模型，采用α = 0.1的标签平滑，但在解码时不采用上述偏差修正，即δ = 0。</p></li><li><h3 id="主实验结果及分析">主实验结果及分析</h3></li><li><p>所有模型解码时不采用长度惩罚（因为这会使上述推导的T项被除去）</p></li><li><p>结果1：</p><ul><li>在EnDe数据集上BLEU值与δ、beam size的关系如下：</li></ul></li></ul><p><img src="https://i1lqvqtgqs.feishu.cn/space/api/box/stream/download/asynccode/?code=MzI3NzhiMjVjOGI3M2JkM2U3ZWZmOTU2NjZjMzE1ODVfaGpoQ3dkSU5KdVRhMjBWUjQ4cDJkMU1yZ2NnSlRyMlJfVG9rZW46Ym94Y25aV1pVN1VISnBwSE1KbGFDTlU3SDZiXzE2NjEzNDg5NTQ6MTY2MTM1MjU1NF9WNA" alt="img"></p><ul><li><p>可以看到修正对贪心搜索没有影响，而随着k增大，修正的影响更大，在beam size = 200时，修正在横轴范围内使得BLEU值持续增大。</p></li><li><p>结果2：</p><ul><li>在EnDe数据集上，当beam size = 200时，BLEU值与源句子长度，是否标签平滑、修正程度的关系如下：</li></ul></li></ul><p><img src="https://i1lqvqtgqs.feishu.cn/space/api/box/stream/download/asynccode/?code=MTA3MjE0ZDcyMDYwZjU0ZTg0ZThkYWE5YTdiZjUyZDFfamJvQlZQSXh1bkY3Tm5KUHZxYk9xc3U1QTJMc1RJZG5fVG9rZW46Ym94Y25sWGFOTHFzMzl1cmFGWU15cDhiVUdlXzE2NjEzNDg5NTQ6MTY2MTM1MjU1NF9WNA" alt="img"></p><ul><li><p>可以看到没有标签平滑的模型在较长查询中性能优于标签平滑模型。而修正后也能让较长查询的结果更好。</p></li><li><p>结果3：</p><ul><li>在EnDe数据集上，长度比值和源句子长度、δ的关系如下：</li></ul></li></ul><p><img src="https://i1lqvqtgqs.feishu.cn/space/api/box/stream/download/asynccode/?code=NWQ0NDZkYjJiNzRhZjUzNDVkNmVjNTlkMmEyYzIyNWFfcDhnYllCQ0dsRGVwcTR3d0tUM1JmQVdXV3QyZGZxQk5fVG9rZW46Ym94Y25kVTFJWjc4aHp6UXI3dmcxaWtXUXFnXzE2NjEzNDg5NTQ6MTY2MTM1MjU1NF9WNA" alt="img"></p><ul><li><p>可以看到没有标签平滑、修正后的标签平滑均有利于生成更长的句子，且有利于更长的查询。</p></li><li><p>结果4：</p><ul><li>不同数据集上的结果（BLEU值）：</li></ul></li></ul><p><img src="https://i1lqvqtgqs.feishu.cn/space/api/box/stream/download/asynccode/?code=NTZiMWIzZGFhZTA2YzQzOTkzNjdlNmI0YjkyNTY5MjRfY1ZyN0tuSTc1VGdkV3ptV2xLZkhwZ01zdzdiN2daY0JfVG9rZW46Ym94Y25GanoyWGg1T3hNRlNQVkZxd2lRYXNmXzE2NjEzNDg5NTQ6MTY2MTM1MjU1NF9WNA" alt="img"></p><ul><li><p>可以看到beam size为4时，δ = 1 / V是一个峰值，beam size为200时，随δ上升模型性能都有上升。这与预期相符。</p></li><li><h3 id="副实验设置、结果、分析">副实验设置、结果、分析</h3></li><li><p>副实验主要探究修正后是否会让模型过度自信（因为设置标签平滑就是为了防止过拟合）。</p></li><li><p>数据集和模型：仍采用上述数据集和模型</p></li><li><p>评测指标：Set-Level Calibration analysis。对于每个查询，将前 200 个beam搜索输出的预测概率相加，用 S 表示，并将参考句子包含在S中的实际频率进行比较，模型校准越好，两个数字越匹配。</p></li><li><p>结果如下：</p></li></ul><p><img src="https://i1lqvqtgqs.feishu.cn/space/api/box/stream/download/asynccode/?code=NGJhNDQyOWMxODY2MDkxNTIxZjQxYWQzNzVmMzRjZGNfYnEwZXFMMERYWUlrYm00bnM4YXI0M0xSek1YRWNVaTNfVG9rZW46Ym94Y25aN1dBVDZVYmdxNGV3Y2ZMWkw4WUFjXzE2NjEzNDg5NTQ6MTY2MTM1MjU1NF9WNA" alt="img"></p><p>标签平滑且无修正的系统过于自信，而经过修正后可以非常匹配。</p><h2 id="个人点评和启发">个人点评和启发</h2><p>用理论推导出beam search解码中存在的问题，并通过实验验证。</p><p>我认为最后合适的δ取1 / V暗示着模型在其他地方还有系统误差（如果只是标签平滑带来的误差，那应该在0.1 / V左右达到最佳）。</p><p>如果是我来修正，根据理论用[0, 1]外的值截断更合理。因为ReLU解决了修正后小于0的概率修正为0的问题，但不会把修正后超过1的概率截断为1，用ReLU后的结果归一化就让人感觉怪怪的。不过由于修正后概率超过1的其实很少（至多有1个，至少有0个），所以应该不会特别影响实验结果。</p>]]></content>
    
    
    <categories>
      
      <category>论文笔记</category>
      
      <category>解码方法</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>解码方法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>文章格式模板</title>
    <link href="/2022/08/24/%E6%96%87%E7%AB%A0%E6%A0%BC%E5%BC%8F%E8%AE%B0%E5%BD%95/"/>
    <url>/2022/08/24/%E6%96%87%E7%AB%A0%E6%A0%BC%E5%BC%8F%E8%AE%B0%E5%BD%95/</url>
    
    <content type="html"><![CDATA[<h1>模板</h1><h2 id="Front-Matter">Front-Matter</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs markdown">---<br><span class="hljs-section">#冒号后都要加空格！</span><br>title: 文章标题<br>tags: [Hexo, Fluid]  #标签<br>index<span class="hljs-emphasis">_img: /img/example.jpg   #首页展示的文章图</span><br><span class="hljs-emphasis">banner_</span>img: /img/post<span class="hljs-emphasis">_banner.jpg #文章上部大图</span><br><span class="hljs-emphasis">date: 2019-10-10 10:00:00 #优先根据 front-matter 里 date 字段，其次是 md 文件日期</span><br><span class="hljs-emphasis">updated: 2019-10-10 10:00:01 #指定更新时间</span><br><span class="hljs-emphasis">excerpt: 摘要</span><br><span class="hljs-emphasis"></span><br><span class="hljs-emphasis">categories:</span><br><span class="hljs-emphasis">- [Diary, PlayStation]</span><br><span class="hljs-emphasis">- [Diary, Games]</span><br><span class="hljs-emphasis">- [Life]</span><br><span class="hljs-emphasis"></span><br><span class="hljs-emphasis">hide: true #在首页隐藏，且会使文章在分类和标签类里都不显示</span><br><span class="hljs-emphasis">sticky: 100 #数值越大，该文章越靠前，达到类似于置顶的效果</span><br><span class="hljs-emphasis"></span><br><span class="hljs-emphasis">---</span><br><span class="hljs-emphasis">以下是文章内容</span><br></code></pre></td></tr></table></figure><p>手动指定摘要的另一种方式，使用 <code>&lt;!-- more --&gt;</code> MD文档里划分，如：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs markdown">正文的一部分作为摘要<br>&lt;!-- more --&gt;<br>余下的正文<br></code></pre></td></tr></table></figure><h2 id="正文">正文</h2><h3 id="脚注">脚注</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs markdown">这是一句话 [^1]<br>[<span class="hljs-symbol">^1</span>]: <span class="hljs-link">这是对应的脚注</span><br></code></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs markdown">正文<br><br><span class="hljs-section">## 参考</span><br>[<span class="hljs-symbol">^1</span>]: <span class="hljs-link">参考资料1</span><br>[<span class="hljs-symbol">^2</span>]: <span class="hljs-link">参考资料2</span><br></code></pre></td></tr></table></figure><h3 id="勾选框">勾选框</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs markdown">&#123;% cb text, checked?, incline? %&#125;<br></code></pre></td></tr></table></figure><p>依次是文本、是否勾选、后面的文字是否不换行（默认false换行）</p><h3 id="按钮">按钮</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs markdown">&#123;% btn url, text, title %&#125;<br></code></pre></td></tr></table></figure><p>依次是跳转链接、按钮文本、鼠标悬停文本</p><h3 id="组图">组图</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs markdown">&#123;% gi total n1-n2-... %&#125;<br>  ![](<span class="hljs-link">url</span>)<br>  ![](<span class="hljs-link">url</span>)<br>  ![](<span class="hljs-link">url</span>)<br>  ![](<span class="hljs-link">url</span>)<br>  ![](<span class="hljs-link">url</span>)<br>&#123;% endgi %&#125;<br></code></pre></td></tr></table></figure><p>total：图片总数量，对应中间包含的图片 url 数量。<br>n1-n2-…：每行的图片数量，可以省略，默认单行最多 3 张图，求和必须相等于 total，否则按默认样式。</p><h3 id="便签">便签</h3><h4 id="行便签">行便签</h4><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs markdown">&#123;% note success %&#125;<br>文字 或者 <span class="hljs-code">`markdown`</span> 均可<br>&#123;% endnote %&#125;<br></code></pre></td></tr></table></figure><div class="note note-primary">            <p>primary</p>          </div><div class="note note-secondary">            <p>secondary</p>          </div><div class="note note-success">            <p>success</p>          </div><div class="note note-info">            <p>info</p>          </div><div class="note note-danger">            <p>danger</p>          </div><div class="note note-warning">            <p>warning</p>          </div><div class="note note-light">            <p>light</p>          </div><h4 id="行内便签">行内便签</h4><p>注意text不能以@开头。也就是不能@后还接一个@。</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs markdown">&#123;% label primary @text %&#125;<br></code></pre></td></tr></table></figure><span class="label label-primary">primary</span> <span class="label label-default">default</span> <span class="label label-info">info</span> <span class="label label-success">success</span> <span class="label label-warning">warning</span> <span class="label label-danger">danger</span>]]></content>
    
    
    <categories>
      
      <category>Hexo相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2022/08/23/hello-world/"/>
    <url>/2022/08/23/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start">Quick Start</h2><h3 id="Create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Genesis</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
