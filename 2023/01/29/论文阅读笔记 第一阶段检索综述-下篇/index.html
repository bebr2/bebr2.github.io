

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <meta name="baidu-site-verification" content="codeva-wEQoSuRzli" />
  <link rel="apple-touch-icon" sizes="76x76" href="/img/logo.jpg">
  <link rel="icon" href="/img/logo.jpg">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#fec8c9">
  <meta name="author" content="BeBr2">
  <meta name="keywords" content="">
  
    <meta name="description" content="接上篇。">
<meta property="og:type" content="article">
<meta property="og:title" content="论文阅读笔记 第一阶段检索综述-下篇">
<meta property="og:url" content="https://bebr2.com/2023/01/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%20%E7%AC%AC%E4%B8%80%E9%98%B6%E6%AE%B5%E6%A3%80%E7%B4%A2%E7%BB%BC%E8%BF%B0-%E4%B8%8B%E7%AF%87/index.html">
<meta property="og:site_name" content="BeBr2&#39;s Blog">
<meta property="og:description" content="接上篇。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://bebr2.com/img/article_content/2023-01-29/0.png">
<meta property="og:image" content="https://bebr2.com/img/article_content/2023-01-29/1.png">
<meta property="og:image" content="https://bebr2.com/img/article_content/2023-01-29/2.png">
<meta property="og:image" content="https://bebr2.com/img/article_content/2023-01-29/3.png">
<meta property="og:image" content="https://bebr2.com/img/article_content/2023-01-29/4.png">
<meta property="og:image" content="https://bebr2.com/img/article_content/2023-01-29/5.png">
<meta property="og:image" content="https://bebr2.com/img/article_content/2023-01-29/6.png">
<meta property="og:image" content="https://bebr2.com/img/article_content/2023-01-29/7.png">
<meta property="og:image" content="https://bebr2.com/img/article_content/2023-01-29/8.png">
<meta property="og:image" content="https://bebr2.com/img/article_content/2023-01-29/9.png">
<meta property="og:image" content="https://bebr2.com/img/article_content/2023-01-29/10.png">
<meta property="article:published_time" content="2023-01-29T08:37:49.000Z">
<meta property="article:modified_time" content="2023-01-29T08:37:49.000Z">
<meta property="article:author" content="BeBr2">
<meta property="article:tag" content="综述">
<meta property="article:tag" content="IR">
<meta property="article:tag" content="第一阶段检索">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://bebr2.com/img/article_content/2023-01-29/0.png">
  
  
  
  <title>论文阅读笔记 第一阶段检索综述-下篇 - BeBr2&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/KaTeX/0.15.6/katex.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  



  
<link rel="stylesheet" href="/css/mouse.css">



<style>
  html {
  overflow-x:hidden;
  }
</style>

  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"bebr2.com","root":"/","version":"1.9.2","typing":{"enable":true,"typeSpeed":100,"cursorChar":"_","loop":false,"scope":["home"]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"left","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":true,"offset_factor":4},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":"451ae6c38ff2b682fd7894e9e2472e31","google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  
    <!-- Baidu Analytics -->
    <script async>
      if (!Fluid.ctx.dnt) {
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?451ae6c38ff2b682fd7894e9e2472e31";
          var s = document.getElementsByTagName("script")[0];
          s.parentNode.insertBefore(hm, s);
        })();
      }
    </script>
  

  

  

  

  

  

  



  
<meta name="generator" content="Hexo 6.2.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 80vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>BeBr2</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/diary/">
                <i class="iconfont icon-note"></i>
                随记
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/boat.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle">论文阅读笔记 第一阶段检索综述-下篇</span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-01-29 16:37" pubdate>
          2023年1月29日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          5k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          57 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
        <div class="scroll-down-bar">
          <i class="iconfont icon-arrowdown"></i>
        </div>
      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="padding-left: 2rem; margin-right: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">论文阅读笔记 第一阶段检索综述-下篇</h1>
            
            
              <div class="markdown-body">
                
                <p>接上篇。</p>
<span id="more"></span>
<h2 id="语义检索的神经方法">语义检索的神经方法</h2>
<p>早期的深度学习方法集中在rerank阶段，晚近几年开始在第一阶段检索有重大进展。</p>
<h3 id="稀疏检索方法">稀疏检索方法</h3>
<p>用稀疏向量表示每个文档和每个查询，其中只有少量维度处于active状态。解释性更好。</p>
<p>稀疏检索很容易集成到倒排索引中，可以分为两类：</p>
<h4 id="神经加权方案">神经加权方案</h4>
<p>在编码时和传统方法类似，但是在term加权评分时，使用神经方法。一种直接的方法是直接预测权重，另一种是用额外的term扩充每个文档，然后用基于term的经典方法来索引（？）。</p>
<p>预测权重的最早模型是DeepTR，用embedding来预测term的重要性，将每一个term的嵌入用回归方法投射到权重上，权重用于评分加权（<em>看文章的意思也可能是在生成query的表示时的加权？</em>）。近来的term discrimination values (TDVs)方法，利用FastText（一个简单快速的浅层神经网络文本分类器）替换倒排索引时的IDF字段，并且最小化document的词袋表示的L1范数，以减少倒排索引的内存占用（？为什么会）。还有人使用上述的翻译语言模型中的embedding，利用单词的嵌入来估计翻译的概率（相当于嵌入越相似，翻译概率越高），弥补了词汇不匹配的问题。</p>
<p>近年来的上下文词嵌入，比以前的静态嵌入（例如经典的Word2Vec）更有进步，所以有工作利用上下文词嵌入来估计加权。基于BERT的DeepCT模型，利用BERT输出的表示映射到term的权重，替换掉倒排索引的TF字段，能够很好地估计词的重要性。对于长文档的term权重，引入HDCT模型，先用BERT生成的表示来估计段落级的术语权重，然后把段落级权重加权和得到文档级权重（这个加权和的权重怎么来？）。</p>
<p>有的工作直接评估term和整个文档的分数。例如Mitra的工作，假设了query term的独立性，使用神经排名模型（如BERT），离线地计算所有词和每个文档之间的分数。在段落级的检索任务比传统方法有显著改进，而比普通的神经方法只差了一点。</p>
<p>除了<strong>显式</strong>预测权重，也有使用seq2seq模型来扩充文档。例如doc2query模型是训练出来的用doc生成query的模型，为每个文档生成几个query，附加在原始文档上，照常索引。docTTTTTquery模型，使用T5来生成query。</p>
<h4 id="稀疏表示学习">稀疏表示学习</h4>
<p>直接学习φ和Φ函数，为q和d构建稀疏向量。学习的稀疏表示可以用于倒排索引，倒排索引表中每一单元代表一个“潜在单词”，而不是传统的term。</p>
<p>一种方法是，将query和document中的每个n-gram映射到低维密集向量，然后学习一个把n-gram表示转换为高维稀疏向量的函数，最后使用点积来计算query和document的相似度。但是n-gram只是局部信息，并不是全局的上下文信息。另外还有UHD-BERT模型。</p>
<h3 id="密集检索方法">密集检索方法</h3>
<p>密集向量能更好地捕捉上下文信息。</p>
<p>通常使用如下的双编码器结构，学习的密集表示通常以近似最近邻算法ANN进行搜索：</p>
<p><img src="/img/article_content/2023-01-29/0.png" srcset="/img/loading.gif" lazyload alt="两个编码器是独立学习的"></p>
<p>重要的是文档端的学习，密集检索方法可以分为term级和doc级：</p>
<p><img src="/img/article_content/2023-01-29/1.png" srcset="/img/loading.gif" lazyload alt="注意，右侧document-encoder最顶层，个数不是文档的term数量"></p>
<h4 id="term-level">term-level</h4>
<p>query和document都表示为term embedding的序列。</p>
<p>最简单的方法是采用词嵌入，更高级的方法有：在大型未标注查询语料库上训练一个word2vec，保留了输入和输出映射（<em>这里很不理解，word2vec确实有两个矩阵，但是第二个矩阵是N*V维的，是将词向量投射到输出的，难道意思是把他转置或取伪逆？？？</em>），在ranking阶段，将query词映射到输入空间，将文档词映射到输出空间，然后聚合q-d之间的余弦相似度，可以对返回的top文档进行rerank。</p>
<p>近来大模型的加入，例如DC-BERT和ColBERT，示意图如下。对query的编码是在线的，而对文档的编码是离线后缓存着的。<strong>DC-BERT</strong>后续使用预训练好的Transformer的后k层进行投影。<strong>ColBERT</strong>使用了交互函数MaxSim（maximum similarity），即针对每一个query term，计算它和所有document term的相似度（余弦相似度或L2距离）的最大值，然后对所有term加和。可以实现廉价高效检索 top-k 相关文档，相较于其他BERT模型速度快、精度差不多。另一个<strong>COIL</strong>模型与其类似，但在计算MaxSim时只与文档中完全匹配的词计算。</p>
<p><img src="/img/article_content/2023-01-29/2.png" srcset="/img/loading.gif" lazyload alt=""></p>
<p>另外，DeFormer和PreTTR旨在分解底层的BERT，查了一下DeFormer，大致是这样：由于Transformer 的低层（lower layers）编码主要关注一些局部的语言表层特征（词形、语法等等），到高层（upper layers）才开始逐渐编码与下游任务相关的全局语义信息。所以可以在前k层对文档编码离线计算得到第 k 层表征，query的第k层表征通过实时计算得到，然后拼接query和文档的表征输入到后面k+1到n层。所以可以加速。下面这幅图示意了DeFormer的计算过程：</p>
<p><img src="/img/article_content/2023-01-29/3.png" srcset="/img/loading.gif" lazyload alt=""></p>
<p>一个自然的扩展是文档的phrase-level（n-gram），而query被视为一个phrase，也就是query-encoder只输出一个embedding。计算相似性时就是query和doc的所有phrase计算。Seo等人提出为OpenQA服务的使用BiLSTM来学习phrase表示，进一步有人将其换为BERT。另外还有句子level的，此处不表。</p>
<h4 id="document-level">document-level</h4>
<p>为每个查询和<strong>每个文档</strong>学习一个或<strong>多个</strong>粗略的全局表示。</p>
<p><strong>单个表示：</strong></p>
<p>最早的Fisher Vector（FV）是使用预定义的启发式函数直接映射单词嵌入，但是性能并不优于经典的IR，还有利用单词嵌入的均值来表示整体，优于基于term的模型。但是这些方法丢失了上下文信息，段落向量法（PV）可以从变长的文本学习固定长度的表示。以上的方法的改进都是有限或局部的。</p>
<p>神经向量空间模型（NVSM），无监督地从零学习词和文档的表示，然后对query的词表示取平均作为query表示，投影到文档的特征空间，然后用余弦相似度计算分数。另一种SAFIR是联合学习词、概念和文档的表示，和NVSM的区别是query的表示是用词和概念的平均来算的。在对嵌入的训练中，也有使用先验的图关系来约束嵌入的相似性的工作，能得到更好的嵌入。</p>
<p>另一种方法是把复杂模型（例如term-level的模型）<strong>蒸馏</strong>为文档级的模型。例如有人把ColBERT计算MaxSim这一步蒸馏为简单的点积，从而实现简单的ANN搜索。</p>
<p>值得注意的是，早期为rerank提出的模型，也能学习高度抽象的文档表示，所以理论上可以用于第一阶段检索，但实验结果是这些模型在整个document训练的效果甚至比只在标题上训练的效果差，这显然不行。所以第一阶段检索阶段需要有专门设计的模型。</p>
<p><strong>多个表示：</strong></p>
<p>也就是文档的编码器会获得多个内容嵌入，而query编码器只获得一个嵌入。原因是文档通常很长且有不同侧面，而query一般很短且只有一个主题。Multi-Vector BERT就是这种，query输入后，只取[CLS]的输出作为嵌入，而document会取前m个（小于文档长度）作为嵌入，然后评分就是query embedding和每个document embedding的内积的最大值。</p>
<p>另一个Poly-encoders，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mrow><mi>c</mi><mi>a</mi><mi>n</mi><mi>d</mi><mi>i</mi></mrow></msub></mrow><annotation encoding="application/x-tex">y_{candi}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">an</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>（我的理解就是query）会参与文档的attention计算：</p>
<p><img src="/img/article_content/2023-01-29/4.png" srcset="/img/loading.gif" lazyload alt=""></p>
<h3 id="混合检索方法">混合检索方法</h3>
<p>稀疏检索方法以单词或“潜在单词”为索引单位，通过每个单位之间的正确匹配来计算分数，从而保持了较强的区分能力。因此，它们可以识别精确匹配，这对于检索任务非常重要。另一方面，密集检索方法学习连续嵌入来编码语义信息，单位的匹配是软的，但丢失了基础的检索功能。混合检索旨在拥有二者的优点，其结构一般如下：</p>
<p><img src="/img/article_content/2023-01-29/5.png" srcset="/img/loading.gif" lazyload alt=""></p>
<p>GLM是使用翻译模型和传统语言模型线性组合的模型，过程没太看懂qwq。。。。</p>
<p>结论是组合模型总是优于传统的语言模型。</p>
<p>有一些工作尝试使用简单神经网络来学习稀疏和密集表示，BOW-CNN是为了检索类似query而产生的，结构如下，也就是在BOW的基础上加了密集向量CNN的信息：</p>
<p><img src="/img/article_content/2023-01-29/6.png" srcset="/img/loading.gif" lazyload alt=""></p>
<p>另一种方法，将query学习为稀疏向量，文档学习为密集向量，使用点积计算相似度。</p>
<p>随着预训练模型的兴起，自然的想法是将其与term-based的方法结合。Seo提出了DenSPI，对于phrase，密集向量使用BERT-based token的开始和末尾embedding，稀疏向量则计算2-gram的tf-dif。Lee等人用BERT取代DenSPI基于词频的稀疏向量计算，使用rectified self-attention间接学习。另外有人简单地将BM25和神经方法的分数线性组合（组合的超参数是经过训练的），也能得到很好的效果。总之，混合方法的有效性是明显的。</p>
<h2 id="模型的训练">模型的训练</h2>
<h3 id="损失函数">损失函数</h3>
<p>最简单的是交叉熵损失（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>d</mi><mo>+</mo></msup></mrow><annotation encoding="application/x-tex">d^+</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7713em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7713em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">+</span></span></span></span></span></span></span></span></span></span></span>是正样本，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>D</mi><mo>−</mo></msup></mrow><annotation encoding="application/x-tex">D^-</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7713em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7713em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">−</span></span></span></span></span></span></span></span></span></span></span>是负样本的集合）：</p>
<p><img src="/img/article_content/2023-01-29/7.png" srcset="/img/loading.gif" lazyload alt=""></p>
<p>还有hinge loss（其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mi mathvariant="normal">∣</mi><msup><mi>D</mi><mo>−</mo></msup><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">n=|D^-|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0213em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7713em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">−</span></span></span></span></span></span></span></span><span class="mord">∣</span></span></span></span>，m通常定为1）：</p>
<p><img src="/img/article_content/2023-01-29/8.png" srcset="/img/loading.gif" lazyload alt=""></p>
<h3 id="负采样">负采样</h3>
<p>负样本如何采样是一个问题，直接决定了模型的质量。大致有三类策略：</p>
<h4 id="随机负采样">随机负采样</h4>
<p>在一个batch或整个语料库随机抽样，如果使用batch，batch_size要求较大。但是计算资源限制了batch_size的大小。He等人使用小batch，同时维护一个队列，最老的batch出队，当前batch进队，在队中抽样。</p>
<p>但是对于本任务而言，随机抽样的负样本通常太容易区分，可以限制在embedding接近正样本的空间中抽样。</p>
<h4 id="Static-Hard-Negative-Sampling">Static Hard Negative Sampling</h4>
<p>使用传统检索器如BM25获得topk个文档，在里面随机抽样。但问题是传统检索器和神经检索器的负样本往往不一样，使得训练和测试时遇到的负样本存在严重不匹配。</p>
<h4 id="Dynamic-Hard-Negative-Sampling">Dynamic Hard Negative Sampling</h4>
<p>从模型本身预测的排名靠前的无关文档中随机抽取样本，也就是动态地抽取负样本。但是语料库太大，训练中即时评分不切实际，可以定时地刷新负样本折中。</p>
<h2 id="展望和挑战">展望和挑战</h2>
<p>主要是预训练目标、训练策略、benchmark的公平性、ANN算法等。此处不表。</p>
<h2 id="自己的补充">自己的补充</h2>
<h3 id="TF-IDF算法">TF-IDF算法</h3>
<p>参考了<a target="_blank" rel="noopener" href="https://blog.csdn.net/asialee_bird/article/details/81486700">TF-IDF算法介绍及实现-CSDN</a></p>
<p>TF是词频（term frequency），IDF是逆向文件频率（Inverse document frequency）。</p>
<p>一种加权算法，用以评估一个字词对于一个文件集或一个语料库中的其中一份文件的重要程度。<strong>字词的重要性随着它在该文件中出现的次数成正比增加，但同时会随着它在整个语料库中出现的频率成反比下降。</strong></p>
<p>TF：该term在该文件中出现次数的占比。</p>
<p>IDF：所有文件中出现该term的文件占比的负对数（但是通常在计算占比时候，包含term的文件数会+1，防止对数里的值为0），所以该值越大，该term重要性越大。</p>
<p>所以TF-IDF算法就是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mi>F</mi><mo>∗</mo><mi>I</mi><mi>D</mi><mi>F</mi></mrow><annotation encoding="application/x-tex">TF*IDF</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">TF</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span></span></span></span>。</p>
<h3 id="BM25算法">BM25算法</h3>
<p>这个似乎一直是一个baseline。</p>
<p>一般性公式如下：</p>
<p class="katex-block "><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo><mo>=</mo><munderover><mo>∑</mo><mi>i</mi><mi>n</mi></munderover><msub><mi>W</mi><mi>i</mi></msub><mi>R</mi><mo stretchy="false">(</mo><msub><mi>q</mi><mi>i</mi></msub><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">score(Q,d)=\sum_i^{n}W_iR(q_i,d)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">score</span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">d</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.9291em;vertical-align:-1.2777em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6514em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">d</span><span class="mclose">)</span></span></span></span></span></p>
<p>R函数是一个query-term和document的相关性得分。</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">W_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是权重，一般会用IDF来计算，IDF是计算也各有千秋，不同于上面讲的IDF，也有这么算的：</p>
<p class="katex-block "><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>I</mi><mi>D</mi><mi>F</mi><mo stretchy="false">(</mo><msub><mi>q</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>l</mi><mi>o</mi><mi>g</mi><mfrac><mrow><mi>N</mi><mo>−</mo><mi>n</mi><mo stretchy="false">(</mo><msub><mi>q</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mn>0.5</mn></mrow><mrow><mi>n</mi><mo stretchy="false">(</mo><msub><mi>q</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mn>0.5</mn></mrow></mfrac></mrow><annotation encoding="application/x-tex">IDF(q_i)=log\frac{N-n(q_i)+0.5}{n(q_i)+0.5}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.363em;vertical-align:-0.936em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord">0.5</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord">0.5</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p>N是所有文档数，n是包含某个term的文档数。</p>
<p>R的计算如下：</p>
<p><img src="/img/article_content/2023-01-29/9.png" srcset="/img/loading.gif" lazyload alt="f_i是q_i在d中的频率，qf_i是在q中的频率，dl是d长度，avgdl是所有文档平均长度"></p>
<h3 id="word2vec的训练">word2vec的训练</h3>
<p>参考<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/114538417">深入浅出Word2Vec原理解析 - 知乎 (zhihu.com)</a></p>
<h4 id="CBOW">CBOW</h4>
<p>训练是用上下文预测中间的词。</p>
<p><img src="/img/article_content/2023-01-29/10.png" srcset="/img/loading.gif" lazyload alt="V是词表大小，N是隐藏层维度，C是上下文C个词"></p>
<p>输出的获得是W’ 矩阵乘以隐藏层，再softmax获得。</p>
<p>最终要的矩阵是W矩阵，词向量的获得就是直接用独热编码乘以矩阵（即矩阵的某一行）。这个矩阵也叫look up table。</p>
<h4 id="skip-gram">skip-gram</h4>
<p>反过来，用一个词预测上下文的词，可以用窗口滑过语料库，中间的词是输入，其他词可以和它构成（输入，输出）对。skipgram的训练会有负样本的加入，这里就不细说了。</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" class="category-chain-item">论文笔记</a>
  
  
    <span>></span>
    
  <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%98%B6%E6%AE%B5%E6%A3%80%E7%B4%A2/" class="category-chain-item">第一阶段检索</a>
  
  

  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E7%BB%BC%E8%BF%B0/">#综述</a>
      
        <a href="/tags/IR/">#IR</a>
      
        <a href="/tags/%E7%AC%AC%E4%B8%80%E9%98%B6%E6%AE%B5%E6%A3%80%E7%B4%A2/">#第一阶段检索</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>论文阅读笔记 第一阶段检索综述-下篇</div>
      <div>https://bebr2.com/2023/01/29/论文阅读笔记 第一阶段检索综述-下篇/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>BeBr2</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2023年1月29日</div>
        </div>
      
      
      <div class="license-meta-item">
        <div>许可协议</div>
        <div>
          
            
            
              <a target="_blank" href="https://creativecommons.org/licenses/by-nc/4.0/">
              <span class="hint--top hint--rounded" aria-label="BY - 署名">
                <i class="iconfont icon-by"></i>
              </span>
              </a>
            
              <a target="_blank" href="https://creativecommons.org/licenses/by-nc/4.0/">
              <span class="hint--top hint--rounded" aria-label="NC - 非商业性使用">
                <i class="iconfont icon-nc"></i>
              </span>
              </a>
            
          
        </div>
      </div>
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2023/03/03/%E4%BB%93%E5%BA%93%E4%BB%8B%E7%BB%8D-THU%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD%E5%AF%BC%E8%AE%BA%E5%A4%A7%E4%BD%9C%E4%B8%9A/" title="仓库介绍-THU因果推断导论大作业">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">仓库介绍-THU因果推断导论大作业</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/01/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%20%E7%AC%AC%E4%B8%80%E9%98%B6%E6%AE%B5%E6%A3%80%E7%B4%A2%E7%BB%BC%E8%BF%B0-%E4%B8%8A%E7%AF%87/" title="论文阅读笔记 第一阶段检索综述-上篇">
                        <span class="hidden-mobile">论文阅读笔记 第一阶段检索综述-上篇</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  <article id="comments">
    
  <script type="text/javascript">
    Fluid.utils.loadComments('#comments', function() {
      var light = 'github-light';
      var dark = 'github-dark';
      var schema = document.documentElement.getAttribute('data-user-color-scheme');
      if (schema === 'dark') {
        schema = dark;
      } else {
        schema = light;
      }
      window.UtterancesThemeLight = light;
      window.UtterancesThemeDark = dark;
      var s = document.createElement('script');
      s.setAttribute('src', 'https://utteranc.es/client.js');
      s.setAttribute('repo', 'bebr2/BlogComment');
      s.setAttribute('issue-term', 'pathname');
      
      s.setAttribute('theme', schema);
      s.setAttribute('crossorigin', 'anonymous');
      document.getElementById('comments').appendChild(s);
    })
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


  </article>


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>
  </div>
</div>





  



  



  



  



  


  
  








    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <span>THU<span> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Theme：Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>





  
    
      <script  src="/js/img-lazyload.js" ></script>
    
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      headingSelector : CONFIG.toc.headingSelector || 'h1,h2,h3,h4,h5,h6',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      collapseDepth   : CONFIG.toc.collapseDepth || 0,
      scrollSmooth    : true,
      headingsOffset  : -boardTop
    });
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>

<!-- <script type="text/javascript" src="http://libs.baidu.com/jquery/1.8.3/jquery.js"></script>
<script type="text/javascript" src="http://libs.baidu.com/jquery/1.8.3/jquery.min.js"></script> -->

<!-- <canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" ></canvas> 
<script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script> 
<script type="text/javascript" src="/js/firework.js"></script> -->


<!-- 雪花特效 -->
<script type="text/javascript" src="\js\snow.js"></script>
